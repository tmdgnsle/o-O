import { useState, useCallback, useRef, useEffect } from 'react';
import { useAppSelector } from '@/store/hooks';
import type { ClientMessage, GptNodeSuggestion } from '../../types/voice.types';

interface UseVoiceGptOptions {
  sendMessage: (message: ClientMessage) => void;
  isConnected: boolean;
  onGptDone: (message: { nodes: GptNodeSuggestion[]; timestamp: number }) => void;
  onGptError?: (error: string, rawText?: string) => void;
  onGptChunk?: (content: string) => void;
}

interface TranscriptItem {
  userId: string;
  userName: string;
  text: string;
  timestamp: number;
}

export function useVoiceGpt({
  sendMessage,
  isConnected,
  onGptDone,
  onGptError,
  onGptChunk,
}: UseVoiceGptOptions) {
  const [isRecording, setIsRecording] = useState(false);
  const [transcripts, setTranscripts] = useState<TranscriptItem[]>([]);

  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const currentUser = useAppSelector((state) => state.user.user);
  const isRecordingRef = useRef(isRecording);
  const sendMessageRef = useRef(sendMessage);
  const currentUserRef = useRef(currentUser);
  const onGptErrorRef = useRef(onGptError);
  const onGptChunkRef = useRef(onGptChunk);

  // Refs ÎèôÍ∏∞Ìôî
  useEffect(() => {
    isRecordingRef.current = isRecording;
  }, [isRecording]);

  useEffect(() => {
    sendMessageRef.current = sendMessage;
  }, [sendMessage]);

  useEffect(() => {
    currentUserRef.current = currentUser;
  }, [currentUser]);

  useEffect(() => {
    onGptErrorRef.current = onGptError;
  }, [onGptError]);

  useEffect(() => {
    onGptChunkRef.current = onGptChunk;
  }, [onGptChunk]);

  // Web Speech API Ï¥àÍ∏∞Ìôî (Ìïú Î≤àÎßå Ïã§Ìñâ)
  useEffect(() => {
    console.log('[VoiceGpt] ===== Initializing Web Speech API =====');

    // @ts-ignore - SpeechRecognition may not be in types
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.error('[VoiceGpt] ‚ùå Speech Recognition not supported in this browser');
      console.error('[VoiceGpt] Please use Chrome, Edge, or Safari');
      return;
    }

    console.log('[VoiceGpt] ‚úÖ Speech Recognition API available');

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR';

    console.log('[VoiceGpt] üîß Speech Recognition configured:', {
      continuous: recognition.continuous,
      interimResults: recognition.interimResults,
      lang: recognition.lang,
      maxAlternatives: recognition.maxAlternatives,
    });

    recognition.onresult = (event) => {
      console.log('[VoiceGpt] üìù Speech recognition result event (resultIndex:', event.resultIndex, ', total results:', event.results.length, ')');

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        const isFinal = event.results[i].isFinal;
        const confidence = event.results[i][0].confidence;

        console.log(`[VoiceGpt] Result[${i}]:`, {
          transcript,
          isFinal,
          confidence,
          length: transcript.length,
        });

        if (isFinal && transcript.trim()) {
          console.log('[VoiceGpt] ‚úÖ Final transcript confirmed:', transcript);
          console.log('[VoiceGpt] üë§ Speaker:', currentUserRef.current?.nickname, `(ID: ${currentUserRef.current?.id})`);

          const message: ClientMessage = {
            type: 'gpt-transcript' as const,
            userId: currentUserRef.current?.id.toString() || '',
            userName: currentUserRef.current?.nickname || '',
            text: transcript,
            isFinal: true,
            timestamp: Date.now(),
          };

          console.log('[VoiceGpt] üì§ Sending transcript to server:', message);
          sendMessageRef.current(message);
        } else if (!isFinal) {
          console.log('[VoiceGpt] üîÑ Interim result (not sending):', transcript.substring(0, 30) + '...');
        } else if (!transcript.trim()) {
          console.log('[VoiceGpt] ‚ö†Ô∏è Empty transcript, skipping');
        }
      }
    };

    recognition.onerror = (event) => {
      console.error('[VoiceGpt] ‚ùå Speech recognition error:', event.error);
      console.error('[VoiceGpt] Error details:', {
        error: event.error,
        message: event.message,
        isRecording,
      });

      if (event.error === 'no-speech') {
        console.warn('[VoiceGpt] ‚ö†Ô∏è No speech detected, will retry on next speech...');
      } else if (event.error === 'aborted') {
        console.log('[VoiceGpt] üõë Recognition aborted intentionally');
      } else if (event.error === 'network') {
        console.error('[VoiceGpt] üåê Network error - check internet connection');
        onGptErrorRef.current?.(`ÏùåÏÑ± Ïù∏Ïãù Ïò§Î•ò: ${event.error} (ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî)`);
      } else if (event.error === 'not-allowed') {
        console.error('[VoiceGpt] üö´ Microphone permission denied');
        onGptErrorRef.current?.(`ÏùåÏÑ± Ïù∏Ïãù Ïò§Î•ò: ${event.error} (ÎßàÏù¥ÌÅ¨ Í∂åÌïúÏùÑ ÌóàÏö©Ìï¥Ï£ºÏÑ∏Ïöî)`);
      } else {
        console.error('[VoiceGpt] ‚ùå Unexpected error:', event.error);
        onGptErrorRef.current?.(`ÏùåÏÑ± Ïù∏Ïãù Ïò§Î•ò: ${event.error}`);
      }
    };

    recognition.onend = () => {
      console.log('[VoiceGpt] üîÑ Recognition ended');
      console.log('[VoiceGpt] Current state - isRecording:', isRecordingRef.current);

      // ÎÖπÏùå Ï§ëÏù¥Î©¥ ÏûêÎèôÏúºÎ°ú Ïû¨ÏãúÏûë
      if (isRecordingRef.current) {
        console.log('[VoiceGpt] üîÅ Auto-restarting recognition (continuous mode)...');
        try {
          recognition.start();
          console.log('[VoiceGpt] ‚úÖ Recognition restarted successfully');
        } catch (e) {
          console.error('[VoiceGpt] ‚ùå Failed to restart recognition:', e);
          console.error('[VoiceGpt] Error details:', {
            name: (e as Error).name,
            message: (e as Error).message,
          });
        }
      } else {
        console.log('[VoiceGpt] ‚èπÔ∏è Not restarting (recording stopped by user)');
      }
    };

    recognitionRef.current = recognition;
    console.log('[VoiceGpt] ‚úÖ Speech Recognition initialized and ready');

    return () => {
      console.log('[VoiceGpt] üßπ Cleaning up Speech Recognition');
      if (recognitionRef.current) {
        recognitionRef.current.stop();
        console.log('[VoiceGpt] ‚úÖ Speech Recognition stopped and cleaned up');
      }
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []); // Ìïú Î≤àÎßå Ïã§Ìñâ (mount/unmount ÏãúÏóêÎßå)

  // ÎÖπÏùå ÏãúÏûë
  const startRecording = useCallback(() => {
    console.log('[VoiceGpt] ===== Starting GPT Recording =====');
    console.log('[VoiceGpt] Connection state:', isConnected);
    console.log('[VoiceGpt] Current user:', currentUser?.nickname, `(ID: ${currentUser?.id})`);

    if (!isConnected) {
      console.error('[VoiceGpt] ‚ùå Not connected to voice server');
      onGptError?.('ÏùåÏÑ± ÏÑúÎ≤ÑÏóê Ïó∞Í≤∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§');
      return;
    }

    if (!recognitionRef.current) {
      console.error('[VoiceGpt] ‚ùå Speech recognition not initialized');
      onGptError?.('ÏùåÏÑ± Ïù∏ÏãùÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§');
      return;
    }

    // ÏÑúÎ≤ÑÏóê ÏãúÏûë Ïã†Ìò∏
    const startMessage: ClientMessage = {
      type: 'gpt-start-recording' as const,
      userId: currentUser?.id.toString() || '',
    };
    console.log('[VoiceGpt] üì§ Sending start message to server:', startMessage);
    sendMessage(startMessage);

    // Web Speech API ÏãúÏûë
    try {
      recognitionRef.current.start();
      setIsRecording(true);
      setTranscripts([]);
      console.log('[VoiceGpt] ‚úÖ Web Speech API started successfully');
      console.log('[VoiceGpt] üé§ Listening for speech...');
    } catch (error) {
      console.error('[VoiceGpt] ‚ùå Failed to start recognition:', error);
      onGptError?.('ÏùåÏÑ± Ïù∏ÏãùÏùÑ ÏãúÏûëÌï† Ïàò ÏóÜÏäµÎãàÎã§');
    }
  }, [isConnected, sendMessage, currentUser, onGptError]);

  // ÎÖπÏùå Ï¢ÖÎ£å
  const stopRecording = useCallback(() => {
    console.log('[VoiceGpt] ===== Stopping GPT Recording =====');
    console.log('[VoiceGpt] Current user:', currentUser?.nickname, `(ID: ${currentUser?.id})`);

    // ÏÑúÎ≤ÑÏóê Ï¢ÖÎ£å Ïã†Ìò∏
    const stopMessage: ClientMessage = {
      type: 'gpt-stop-recording' as const,
      userId: currentUser?.id.toString() || '',
    };
    console.log('[VoiceGpt] üì§ Sending stop message to server:', stopMessage);
    sendMessage(stopMessage);

    // Web Speech API Ï¢ÖÎ£å
    if (recognitionRef.current) {
      console.log('[VoiceGpt] üõë Stopping Web Speech API...');
      recognitionRef.current.stop();
    } else {
      console.warn('[VoiceGpt] ‚ö†Ô∏è Recognition ref is null, cannot stop');
    }

    setIsRecording(false);
    console.log('[VoiceGpt] ‚úÖ Recording stopped successfully');
  }, [sendMessage, currentUser]);

  // Îã§Î•∏ ÏÇ¨Îûå transcript Ï∂îÍ∞Ä
  const addPeerTranscript = useCallback(
    (userId: string, userName: string, text: string, timestamp: number) => {
      console.log('[VoiceGpt] üë• Adding peer transcript:', {
        userId,
        userName,
        text,
        timestamp: new Date(timestamp).toISOString(),
      });

      setTranscripts((prev) => {
        const newTranscripts = [...prev, { userId, userName, text, timestamp }];
        console.log('[VoiceGpt] üìã Total transcripts:', newTranscripts.length);
        return newTranscripts;
      });
    },
    []
  );

  // GPT Ï≤≠ÌÅ¨ Ìï∏Îì§Îü¨
  const handleGptChunk = useCallback((content: string) => {
    console.log('[VoiceGpt]', content);
    onGptChunkRef.current?.(content);
  }, []);

  // ÎÖπÏùå ÌÜ†Í∏Ä
  const toggleRecording = useCallback(() => {
    if (isRecording) {
      stopRecording();
    } else {
      startRecording();
    }
  }, [isRecording, startRecording, stopRecording]);

  return {
    isRecording,
    transcripts,
    startRecording,
    stopRecording,
    toggleRecording,
    addPeerTranscript,
    handleGptChunk,
  };
}
