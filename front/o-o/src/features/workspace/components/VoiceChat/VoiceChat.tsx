// components/VoiceChat/VoiceChat.tsx
import React, { useMemo, useEffect, useCallback, useRef } from "react";
import VoiceAvatar from "./VoiceAvatar";
import VoiceControls from "./VoiceContols";
import { useVoiceChat } from "../../hooks/custom/useVoiceChat";
import { useVoiceGpt } from "../../hooks/custom/useVoiceGpt";
import { useGptNodeCreator } from "../../../mindmap/hooks/custom/useGptNodeCreator";
import { usePeerCursors } from "../PeerCursorProvider";
import { useAppSelector } from "@/store/hooks";
import type { NodeData } from "../../../mindmap/types";
import type { GptNodeSuggestion } from "../../types/voice.types";

interface YjsCRUD {
  create: (node: NodeData) => void;
  read: (id: string) => NodeData | undefined;
}

interface VoiceChatProps {
  workspaceId: string;
  crud: YjsCRUD | null;
  myRole?: string;
  onCallEnd?: () => void;
  onOrganize?: () => void;
  onShare?: () => void;
  onGptRecordingChange?: (isRecording: boolean) => void;
  onGptNodesReceived?: (nodes: GptNodeSuggestion[]) => void;
}

const VoiceChat: React.FC<VoiceChatProps> = ({
  workspaceId,
  crud,
  myRole,
  onCallEnd,
  onOrganize,
  onShare,
  onGptRecordingChange,
  onGptNodesReceived,
}) => {
  const currentUser = useAppSelector((state) => state.user.user);
  const { peers } = usePeerCursors();

  // GPT Node Creator
  const { createNodesFromGpt } = useGptNodeCreator(crud, workspaceId);

  // Ref to store handleGptChunk function
  const handleGptChunkRef = useRef<((content: string) => void) | null>(null);

  // GPT ì²­í¬ í•¸ë“¤ëŸ¬
  const onGptChunkReceived = useCallback((content: string) => {
    // useVoiceGptì˜ handleGptChunk í˜¸ì¶œ
    handleGptChunkRef.current?.(content);
  }, []);

  // Use the voice chat hook
  const {
    isInVoice,
    participants,
    isMuted,
    isSpeaking,
    remoteStreams,
    joinVoice,
    leaveVoice,
    toggleMute,
    sendMessage,
    connectionState,
  } = useVoiceChat({
    workspaceId,
    userId: currentUser?.id.toString(),
    enabled: false, // Manual join via button
    onGptChunk: onGptChunkReceived,
    onGptDone: (message) => {
      console.log('[VoiceChat] ===== GPT Processing Complete =====');
      console.log('[VoiceChat] ðŸ“Š Received GPT response:', {
        nodeCount: message.nodes.length,
        timestamp: new Date(message.timestamp).toISOString(),
      });
      console.log('[VoiceChat] ðŸŽ¯ GPT Nodes:', message.nodes);

      // ë…¸ë“œë¥¼ ë§ˆì¸ë“œë§µì— ì¶”ê°€
      createNodesFromGpt(message.nodes);

      // ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì— ë…¸ë“œ ì „ë‹¬ (ExtractedKeywordListì— í‘œì‹œí•˜ê¸° ìœ„í•´)
      onGptNodesReceived?.(message.nodes);

      console.log('[VoiceChat] âœ… GPT ë…¸ë“œ ìƒì„± ì™„ë£Œ ë° ë°ì´í„° ì „ë‹¬');
    },
    onGptError: (message) => {
      console.error('[VoiceChat] ===== GPT Error =====');
      console.error('[VoiceChat] âŒ Error message:', message.error);

      if (message.rawText) {
        console.error('[VoiceChat] ðŸ“„ Raw GPT response (failed to parse):');
        console.error(message.rawText);
        console.error('[VoiceChat] Response length:', message.rawText.length, 'characters');
      } else {
        console.error('[VoiceChat] âš ï¸ No raw response available');
      }

      // TODO: ì‚¬ìš©ìžì—ê²Œ ì—ëŸ¬ ì•Œë¦¼ í‘œì‹œ
      console.log('[VoiceChat] ðŸ’¡ TODO: Display error notification to user');
    },
  });

  // GPT Hook (only for UI state management - handlers are in useVoiceChat)
  const gpt = useVoiceGpt({
    sendMessage,
    isConnected: connectionState === 'connected',
    onGptChunk: () => {}, // Handled by useVoiceChat
    onGptDone: () => {}, // Handled by useVoiceChat
    onGptError: () => {}, // Handled by useVoiceChat
  });

  // handleGptChunkë¥¼ refì— ì €ìž¥
  useEffect(() => {
    handleGptChunkRef.current = gpt.handleGptChunk;
  }, [gpt.handleGptChunk]);

  // GPT ë…¹ìŒ ìƒíƒœ ë³€ê²½ ì‹œ ë¶€ëª¨ì—ê²Œ ì•Œë¦¼
  useEffect(() => {
    onGptRecordingChange?.(gpt.isRecording);
  }, [gpt.isRecording, onGptRecordingChange]);

  // Join voice chat on mount
  useEffect(() => {
    if (!isInVoice) {
      joinVoice();
    }
  }, []); // Only run on mount

  const handleCallToggle = () => {
    if (isInVoice) {
      leaveVoice();
      onCallEnd?.();
    }
  };

  // Play remote audio streams
  useEffect(() => {
    remoteStreams.forEach((stream, userId) => {
      // Find or create audio element for this user
      let audioElement = document.getElementById(`voice-audio-${userId}`) as HTMLAudioElement;

      if (!audioElement) {
        audioElement = document.createElement('audio');
        audioElement.id = `voice-audio-${userId}`;
        audioElement.autoplay = true;
        audioElement.style.display = 'none';
        document.body.appendChild(audioElement);
      }

      if (audioElement.srcObject !== stream) {
        audioElement.srcObject = stream;
      }
    });

    // Cleanup audio elements for users who left
    // Only remove audio elements that are no longer in remoteStreams
    const activeUserIds = new Set(Array.from(remoteStreams.keys()));
    const allAudioElements = document.querySelectorAll('[id^="voice-audio-"]');
    allAudioElements.forEach((element) => {
      const userId = element.id.replace('voice-audio-', '');
      if (!activeUserIds.has(userId)) {
        element.remove();
      }
    });
  }, [remoteStreams]);

  // Build user list with voice states
  const voiceUsers = useMemo(() => {
    // Map participants to user display data
    const users = participants.map((participant, index) => {
      // Check if this participant is current user
      const isCurrentUser = participant.userId === currentUser?.id.toString();

      // Find peer data for avatar/name (for other users)
      const peer = peers.find((p) => p.userId?.toString() === participant.userId);

      // Determine if this participant is speaking
      const isUserSpeaking = isCurrentUser
        ? isSpeaking
        : participant.voiceState?.speaking ?? false;

      return {
        id: participant.userId,
        name: isCurrentUser
          ? currentUser.nickname
          : (peer?.name ?? `User ${participant.userId}`),
        avatar: isCurrentUser
          ? (currentUser.profileImage ?? '')
          : (peer?.profileImage ?? ''),
        isSpeaking: isUserSpeaking && !(participant.voiceState?.muted ?? false),
        colorIndex: index % 6,
      };
    });

    return users;
  }, [participants, peers, currentUser, isSpeaking]);

  return (
    <div
      className="flex items-center gap-3 bg-semi-white px-4 py-2 shadow-lg"
      style={{ borderRadius: "20px" }}
    >
      {/* Users Avatars */}
      <div className="flex items-center gap-3">
        {voiceUsers.map((user, index) => (
          <VoiceAvatar
            key={user.id}
            avatar={user.avatar}
            name={user.name}
            isSpeaking={user.isSpeaking}
            colorIndex={user.colorIndex}
            index={index}
          />
        ))}
      </div>

      {/* Divider */}
      <div className="h-12 w-px bg-gray-300 mx-2" />

      {/* Control Buttons */}
      <VoiceControls
        isMuted={isMuted}
        isCallActive={isInVoice}
        isGptRecording={gpt.isRecording}
        showOrganize={myRole === "MAINTAINER"}
        onMicToggle={toggleMute}
        onCallToggle={handleCallToggle}
        onOrganize={gpt.toggleRecording}
        onShare={onShare}
      />
    </div>
  );
};

export default VoiceChat;
