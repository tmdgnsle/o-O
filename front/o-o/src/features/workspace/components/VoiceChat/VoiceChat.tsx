// components/VoiceChat/VoiceChat.tsx
import React, { useMemo, useEffect, useCallback, useRef } from "react";
import VoiceAvatar from "./VoiceAvatar";
import VoiceControls from "./VoiceContols";
import { useVoiceChat } from "../../hooks/custom/useVoiceChat";
import { useVoiceGpt } from "../../hooks/custom/useVoiceGpt";
import { useGptNodeCreator } from "../../../mindmap/hooks/custom/useGptNodeCreator";
import { usePeerCursors } from "../PeerCursorProvider";
import { useAppSelector } from "@/store/hooks";
import type { NodeData } from "../../../mindmap/types";
import type { GptNodeSuggestion } from "../../types/voice.types";
import { getProfileImageUrl } from "@/shared/utils/imageMapper";
import type { YClient } from "../../hooks/custom/yjsClient";
import ConfirmDialog from "@/shared/ui/ConfirmDialog";
import ContentDialog from "@/shared/ui/ContentDialog/ContentDialog";
import popoImage from "@/shared/assets/images/organize_popo.webp";
import { useWorkspaceAccessQuery } from "../../hooks/query/useWorkspaceAccessQuery";

interface YjsCRUD {
  create: (node: NodeData) => void;
  read: (id: string) => NodeData | undefined;
}

interface VoiceChatProps {
  workspaceId: string;
  crud: YjsCRUD | null;
  nodes?: NodeData[];
  myRole?: string;
  onCallEnd?: () => void;
  onOrganize?: () => void;
  onGptRecordingChange?: (isRecording: boolean) => void;
  onGptNodesReceived?: (nodes: GptNodeSuggestion[], createdNodeIds: string[]) => void;
  onGptToggleReady?: (toggle: () => void) => void;
  yclient?: YClient | null;
  cursorColor?: string;
  gptState?: {
    isRecording: boolean;
    keywords: Array<{ id: string; label: string; children?: any[] }>;
    startedBy: string;
    timestamp: number;
  } | null;
  updateGptState?: (gptData: {
    isRecording: boolean;
    keywords: Array<{ id: string; label: string; children?: any[] }>;
    startedBy: string;
    timestamp: number;
  } | null) => void;
}

const VoiceChat: React.FC<VoiceChatProps> = ({
  workspaceId,
  crud,
  nodes = [],
  myRole,
  onCallEnd,
  onOrganize,
  onGptRecordingChange,
  onGptNodesReceived,
  onGptToggleReady,
  yclient,
  cursorColor,
  gptState,
  updateGptState,
}) => {
  const currentUser = useAppSelector((state) => state.user.user);
  const { peers } = usePeerCursors();
  const { workspace } = useWorkspaceAccessQuery(workspaceId);

  // GPT Node Creator
  const { createNodesFromGpt } = useGptNodeCreator(crud, workspaceId, nodes, workspace?.theme ?? "PASTEL");

  // Ref to store handleGptChunk function
  const handleGptChunkRef = useRef<((content: string) => void) | null>(null);

  // Ref to prevent duplicate GPT processing
  const processedGptTimestamps = useRef(new Set<number>());

  // Refs for GPT control functions (will be set after gpt is initialized)
  const gptStartRecordingRef = useRef<(() => void) | null>(null);
  const gptStopRecordingRef = useRef<(() => void) | null>(null);
  const gptIsRecordingRef = useRef<boolean>(false);

  // Ref to track previous recording state
  const prevIsRecordingRef = useRef<boolean>(false);

  // Refs for stable access in callbacks (avoid dependency array changes)
  const currentUserRef = useRef(currentUser);
  const gptStateRef = useRef(gptState);
  const createNodesFromGptRef = useRef(createNodesFromGpt);

  // Sync refs with latest values
  useEffect(() => {
    currentUserRef.current = currentUser;
  }, [currentUser]);

  useEffect(() => {
    gptStateRef.current = gptState;
  }, [gptState]);

  useEffect(() => {
    createNodesFromGptRef.current = createNodesFromGpt;
  }, [createNodesFromGpt]);

  // GPT ì²­í¬ í•¸ë“¤ëŸ¬
  const onGptChunkReceived = useCallback((content: string) => {
    // useVoiceGptì˜ handleGptChunk í˜¸ì¶œ
    handleGptChunkRef.current?.(content);
  }, []);

  // GPT Done í•¸ë“¤ëŸ¬ (useCallbackìœ¼ë¡œ memoization)
  const handleGptDone = useCallback((message: { nodes: any[]; timestamp: number }) => {
    // ì¤‘ë³µ ë°©ì§€: ê°™ì€ timestampë¥¼ 2ë²ˆ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
    if (processedGptTimestamps.current.has(message.timestamp)) {
      console.warn('[VoiceChat] âš ï¸ Duplicate GPT done message ignored:', message.timestamp);
      return;
    }
    processedGptTimestamps.current.add(message.timestamp);

    console.log('[VoiceChat] ===== GPT Processing Complete =====');
    console.log('[VoiceChat] ðŸ“Š Received GPT response:', {
      nodeCount: message.nodes.length,
      timestamp: new Date(message.timestamp).toISOString(),
    });
    console.log('[VoiceChat] ðŸŽ¯ GPT Nodes:', message.nodes);

    // MAINTAINERë§Œ ë…¸ë“œë¥¼ ìƒì„± (ê¶Œí•œ ê¸°ë°˜ ì œì–´)
    const isMaintainer = myRole === 'MAINTAINER';

    console.log('[VoiceChat] ðŸ” ê¶Œí•œ ì²´í¬:', {
      myRole,
      isMaintainer,
      currentUserId: currentUserRef.current?.id.toString(),
    });

    let createdNodeIds: string[] = [];

    if (isMaintainer) {
      console.log('[VoiceChat] ðŸŽ¯ MAINTAINER â†’ ë…¸ë“œ ìƒì„±');
      // ë…¸ë“œë¥¼ ë§ˆì¸ë“œë§µì— ì¶”ê°€í•˜ê³  ìƒì„±ëœ ë…¸ë“œ IDë“¤ ë°›ê¸° (refë¡œ ìµœì‹  í•¨ìˆ˜ ì°¸ì¡°)
      createdNodeIds = createNodesFromGptRef.current(message.nodes);
    } else {
      console.log('[VoiceChat] â„¹ï¸ ë‹¤ë¥¸ ì—­í•  â†’ í‚¤ì›Œë“œ í‘œì‹œë§Œ');
    }

    // ëª¨ë“  ì°¸ì—¬ìž: ë¶€ëª¨ ì»´í¬ë„ŒíŠ¸ì— ë…¸ë“œì™€ ìƒì„±ëœ IDë“¤ ì „ë‹¬ (ExtractedKeywordListì— í‘œì‹œí•˜ê¸° ìœ„í•´)
    onGptNodesReceived?.(message.nodes, createdNodeIds);

    console.log('[VoiceChat] âœ… GPT ì²˜ë¦¬ ì™„ë£Œ');
  }, [onGptNodesReceived, myRole]);

  // GPT Error í•¸ë“¤ëŸ¬ (useCallbackìœ¼ë¡œ memoization)
  const handleGptError = useCallback((message: { error: string; rawText?: string; timestamp: number }) => {
    console.error('[VoiceChat] ===== GPT Error =====');
    console.error('[VoiceChat] âŒ Error message:', message.error);

    if (message.rawText) {
      console.error('[VoiceChat] ðŸ“„ Raw GPT response (failed to parse):');
      console.error(message.rawText);
      console.error('[VoiceChat] Response length:', message.rawText.length, 'characters');
    } else {
      console.error('[VoiceChat] âš ï¸ No raw response available');
    }

    // TODO: ì‚¬ìš©ìžì—ê²Œ ì—ëŸ¬ ì•Œë¦¼ í‘œì‹œ
    console.log('[VoiceChat] ðŸ’¡ TODO: Display error notification to user');
  }, []);

  // GPT Recording Started í•¸ë“¤ëŸ¬ (ref ì‚¬ìš©)
  const handleGptRecordingStarted = useCallback((startedBy: string, timestamp: number) => {
    console.log('[VoiceChat] ===== GPT ë…¹ìŒ ì‹œìž‘ë¨ =====');
    console.log('[VoiceChat] ðŸ‘¤ Started by:', startedBy);
    console.log('[VoiceChat] ðŸ• Timestamp:', new Date(timestamp).toISOString());
    console.log('[VoiceChat] ðŸ“Š Current user:', currentUserRef.current?.id.toString());

    // ë‹¤ë¥¸ ì‚¬ëžŒì´ ë…¹ìŒì„ ì‹œìž‘í•œ ê²½ìš°, ìžë™ìœ¼ë¡œ STT ì‹œìž‘
    if (startedBy !== currentUserRef.current?.id.toString()) {
      console.log('[VoiceChat] ðŸŽ¤ ë‹¤ë¥¸ ì‚¬ìš©ìžê°€ ë…¹ìŒ ì‹œìž‘ â†’ ìžë™ìœ¼ë¡œ STT ì‹œìž‘');
      if (!gptIsRecordingRef.current) {
        gptStartRecordingRef.current?.();
      } else {
        console.log('[VoiceChat] âš ï¸ ì´ë¯¸ ë…¹ìŒ ì¤‘, ìŠ¤í‚µ');
      }
    } else {
      console.log('[VoiceChat] â„¹ï¸ ë³¸ì¸ì´ ì‹œìž‘í•œ ë…¹ìŒ, STTëŠ” ì´ë¯¸ ì‹œìž‘ë¨');
    }
  }, []);

  // GPT Session Ended í•¸ë“¤ëŸ¬ (ref ì‚¬ìš©)
  const handleGptSessionEnded = useCallback(() => {
    console.log('[VoiceChat] ===== GPT ì„¸ì…˜ ì¢…ë£Œë¨ =====');

    if (gptIsRecordingRef.current) {
      console.log('[VoiceChat] ðŸ›‘ ìžë™ìœ¼ë¡œ STT ì¢…ë£Œ...');
      gptStopRecordingRef.current?.();
    } else {
      console.log('[VoiceChat] âš ï¸ ë…¹ìŒ ì¤‘ì´ ì•„ë‹˜, ìŠ¤í‚µ');
    }
  }, []);

  // Meeting Minutes handlers (will be passed to useVoiceChat)
  const [meetingMinutesState, setMeetingMinutesState] = React.useState({
    isGenerating: false,
    content: '',
    showConfirmDialog: false,
    showContentDialog: false,
    error: null as string | null,
  });

  const handleMeetingMinutesChunk = useCallback(
    (message: { content: string; timestamp: number }) => {
      console.log('[VoiceChat] ðŸ“¦ Meeting minutes chunk received');
      setMeetingMinutesState((prev) => ({
        ...prev,
        content: prev.content + message.content,
      }));
    },
    []
  );

  const handleMeetingMinutesDone = useCallback(
    (message: { content: string; timestamp: number }) => {
      console.log('[VoiceChat] âœ… Meeting minutes generation complete');
      setMeetingMinutesState((prev) => ({
        ...prev,
        isGenerating: false,
        content: message.content,
      }));
    },
    []
  );

  const handleMeetingMinutesError = useCallback(
    (message: { error: string; timestamp: number }) => {
      console.error('[VoiceChat] âŒ Meeting minutes error:', message.error);
      setMeetingMinutesState((prev) => ({
        ...prev,
        isGenerating: false,
        error: message.error,
      }));
    },
    []
  );

  // Use the voice chat hook
  const {
    isInVoice,
    participants,
    isMuted,
    isSpeaking,
    remoteStreams,
    joinVoice,
    leaveVoice,
    toggleMute,
    sendMessage,
    connectionState,
    requestMeetingMinutes,
  } = useVoiceChat({
    workspaceId,
    userId: currentUser?.id.toString(),
    enabled: false, // Manual join via button
    onGptChunk: onGptChunkReceived,
    onGptDone: handleGptDone,
    onGptError: handleGptError,
    onGptRecordingStarted: handleGptRecordingStarted,
    onGptSessionEnded: handleGptSessionEnded,
    onMeetingMinutesChunk: handleMeetingMinutesChunk,
    onMeetingMinutesDone: handleMeetingMinutesDone,
    onMeetingMinutesError: handleMeetingMinutesError,
  });

  // GPT Hook (only for UI state management - handlers are in useVoiceChat)
  const gpt = useVoiceGpt({
    sendMessage,
    isConnected: connectionState === 'connected',
    onGptChunk: () => {}, // Handled by useVoiceChat
    onGptDone: () => {}, // Handled by useVoiceChat
    onGptError: () => {}, // Handled by useVoiceChat
  });

  // handleGptChunkë¥¼ refì— ì €ìž¥
  useEffect(() => {
    handleGptChunkRef.current = gpt.handleGptChunk;
  }, [gpt.handleGptChunk]);

  // GPT control functionsë¥¼ refì— ì €ìž¥
  useEffect(() => {
    gptStartRecordingRef.current = gpt.startRecording;
    gptStopRecordingRef.current = gpt.stopRecording;
    gptIsRecordingRef.current = gpt.isRecording;
  }, [gpt.startRecording, gpt.stopRecording, gpt.isRecording]);

  // GPT ë…¹ìŒ ìƒíƒœ ë³€ê²½ ì‹œ ì²˜ë¦¬
  useEffect(() => {
    onGptRecordingChange?.(gpt.isRecording);

    // ë…¹ìŒì´ ì‹œìž‘ë  ë•Œë§Œ Awareness ì´ˆê¸°í™” (false -> true ì „í™˜)
    if (gpt.isRecording && !prevIsRecordingRef.current) {
      if (updateGptState && currentUserRef.current) {
        // console.log('[VoiceChat] ðŸ“¡ Awareness ì´ˆê¸°í™”: ë…¹ìŒ ì‹œìž‘');
        updateGptState({
          isRecording: true,
          keywords: [], // ì´ˆê¸° ìƒíƒœ
          startedBy: currentUserRef.current.id.toString(),
          timestamp: Date.now(),
        });
      }
    }
    // ë…¹ìŒì´ ì¢…ë£Œë  ë•Œ isRecordingë§Œ falseë¡œ ì—…ë°ì´íŠ¸ (í‚¤ì›Œë“œëŠ” ìœ ì§€)
    else if (!gpt.isRecording && prevIsRecordingRef.current) {
      if (updateGptState && gptStateRef.current) {
        // console.log('[VoiceChat] ðŸ“¡ Awareness ì—…ë°ì´íŠ¸: ë…¹ìŒ ì¢…ë£Œ');
        // console.log('[VoiceChat] í˜„ìž¬ ìƒíƒœ:', gptStateRef.current);
        updateGptState({
          ...gptStateRef.current, // ref ì‚¬ìš©ìœ¼ë¡œ ìµœì‹  ìƒíƒœ ì°¸ì¡°
          isRecording: false, // ë…¹ìŒ ìƒíƒœë§Œ falseë¡œ ë³€ê²½
          // keywords, startedBy, timestampëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        });
      }
    }

    // ì´ì „ ìƒíƒœ ì—…ë°ì´íŠ¸
    prevIsRecordingRef.current = gpt.isRecording;
  }, [gpt.isRecording, updateGptState, onGptRecordingChange]); // gptState ì œê±°, ref ì‚¬ìš©

  // GPT toggle í•¨ìˆ˜ë¥¼ ë¶€ëª¨ì—ê²Œ ì „ë‹¬
  useEffect(() => {
    onGptToggleReady?.(gpt.toggleRecording);
  }, [gpt.toggleRecording, onGptToggleReady]);

  // Join voice chat on mount
  useEffect(() => {
    if (!isInVoice) {
      joinVoice();
    }

    // Cleanup on unmount
    return () => {
      console.log('[VoiceChat] ðŸ§¹ Component unmounting, cleaning up...');

      // Stop GPT recording if active
      if (gpt.isRecording) {
        console.log('[VoiceChat] ðŸ›‘ Stopping GPT recording on unmount...');
        gpt.stopRecording();
      }

      // Leave voice chat if active
      if (isInVoice) {
        console.log('[VoiceChat] ðŸ“ž Leaving voice chat on unmount...');
        leaveVoice();
      }
    };
  }, []); // Only run on mount

  const handleCallToggle = () => {
    if (isInVoice) {
      // Stop GPT recording FIRST if active (to stop Web Speech API)
      if (gpt.isRecording) {
        console.log('[VoiceChat] ðŸ›‘ Stopping GPT recording before leaving voice...');
        gpt.stopRecording();
      }

      // MAINTAINER: Show confirm dialog to choose between mindmap or meeting minutes
      const isMaintainer = myRole === 'MAINTAINER';
      if (isMaintainer) {
        console.log('[VoiceChat] ðŸ‘¤ MAINTAINER ending voice â†’ Show confirm dialog');
        setMeetingMinutesState((prev) => ({ ...prev, showConfirmDialog: true }));
      } else {
        // MEMBER: Just leave voice chat immediately
        console.log('[VoiceChat] ðŸ‘¤ MEMBER ending voice â†’ Leave immediately');
        leaveVoice();
        onCallEnd?.();
      }
    }
  };

  // Handle "View Mindmap" button click in confirm dialog
  const handleViewMindmap = useCallback(() => {
    console.log('[VoiceChat] ðŸ“ User chose "View Mindmap" â†’ Cleanup and close');
    setMeetingMinutesState((prev) => ({ ...prev, showConfirmDialog: false }));
    leaveVoice();
    onCallEnd?.();
  }, [leaveVoice, onCallEnd]);

  // Handle "View Meeting Minutes" button click in confirm dialog
  const handleViewMeetingMinutes = useCallback(() => {
    console.log('[VoiceChat] ðŸ“ User chose "View Meeting Minutes" â†’ Generate');
    setMeetingMinutesState((prev) => ({
      ...prev,
      showConfirmDialog: false,
      showContentDialog: true,
      isGenerating: true,
      content: '',
      error: null,
    }));
    requestMeetingMinutes();
  }, [requestMeetingMinutes]);

  // Handle meeting minutes dialog close
  const handleMeetingMinutesClose = useCallback(() => {
    console.log('[VoiceChat] âŒ Closing meeting minutes dialog â†’ Cleanup');
    setMeetingMinutesState((prev) => ({
      ...prev,
      showContentDialog: false,
      content: '',
      error: null,
    }));
    leaveVoice();
    onCallEnd?.();
  }, [leaveVoice, onCallEnd]);

  // Play remote audio streams
  useEffect(() => {
    remoteStreams.forEach((stream, userId) => {
      // Find or create audio element for this user
      let audioElement = document.getElementById(`voice-audio-${userId}`) as HTMLAudioElement;

      if (!audioElement) {
        audioElement = document.createElement('audio');
        audioElement.id = `voice-audio-${userId}`;
        audioElement.autoplay = true;
        audioElement.style.display = 'none';
        document.body.appendChild(audioElement);
      }

      if (audioElement.srcObject !== stream) {
        audioElement.srcObject = stream;
      }
    });

    // Cleanup audio elements for users who left
    // Only remove audio elements that are no longer in remoteStreams
    const activeUserIds = new Set(Array.from(remoteStreams.keys()));
    const allAudioElements = document.querySelectorAll('[id^="voice-audio-"]');
    allAudioElements.forEach((element) => {
      const userId = element.id.replace('voice-audio-', '');
      if (!activeUserIds.has(userId)) {
        element.remove();
      }
    });
  }, [remoteStreams]);

  // Build user list with voice states
  const voiceUsers = useMemo(() => {
    // Map participants to user display data
    const users = participants.map((participant, index) => {
      // Check if this participant is current user
      const isCurrentUser = participant.userId === currentUser?.id.toString();

      // Find peer data for avatar/name (for other users)
      const peer = peers.find((p) => p.userId?.toString() === participant.userId);

      // Determine if this participant is speaking
      const isUserSpeaking = isCurrentUser
        ? isSpeaking
        : participant.voiceState?.speaking ?? false;

      return {
        id: participant.userId,
        name: isCurrentUser
          ? currentUser.nickname
          : (peer?.name ?? `User ${participant.userId}`),
        avatar: isCurrentUser
          ? getProfileImageUrl(currentUser.profileImage)
          : getProfileImageUrl(peer?.profileImage),
        isSpeaking: isUserSpeaking && !(participant.voiceState?.muted ?? false),
        voiceColor: isCurrentUser
          ? cursorColor
          : peer?.color,
        colorIndex: index % 6,
      };
    });

    return users;
  }, [participants, peers, currentUser, isSpeaking, cursorColor]);

  return (
    <>
      <div
        className="flex items-center gap-3 bg-semi-white px-4 py-2 shadow-lg"
        style={{ borderRadius: "20px" }}
      >
        {/* Users Avatars */}
        <div className="flex items-center gap-3">
          {voiceUsers.map((user, index) => (
            <VoiceAvatar
              key={user.id}
              avatar={user.avatar}
              name={user.name}
              isSpeaking={user.isSpeaking}
              voiceColor={user.voiceColor}
              colorIndex={user.colorIndex}
              index={index}
            />
          ))}
        </div>

        {/* Divider */}
        <div className="h-12 w-px bg-gray-300 mx-2" />

        {/* Control Buttons */}
        <VoiceControls
          isMuted={isMuted}
          isCallActive={isInVoice}
          isGptRecording={gpt.isRecording}
          showOrganize={myRole === "MAINTAINER"}
          onMicToggle={toggleMute}
          onCallToggle={handleCallToggle}
          onOrganize={gpt.toggleRecording}
          workspaceId={workspaceId}
          yclient={yclient}
          peers={peers}
        />
      </div>

      {/* Meeting Minutes Dialogs */}
      <ConfirmDialog
        isOpen={meetingMinutesState.showConfirmDialog}
        onClose={() => setMeetingMinutesState((prev) => ({ ...prev, showConfirmDialog: false }))}
        characterImage={popoImage}
        title="íšŒì˜ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        description={`íšŒì˜ ë‚´ìš©ì€ Popoê°€ ì •ë¦¬í•´ë“œë ¸ì–´ìš”.\nìƒì„±ëœ íšŒì˜ë¡ì„ í™•ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?`}
        buttons={[
          {
            id: "view-mindmap",
            text: "ë§ˆì¸ë“œë§µ ë³´ê¸°",
            onClick: handleViewMindmap,
            variant: "outline",
          },
          {
            id: "view-meeting-minutes",
            text: "íšŒì˜ë¡ í™•ì¸í•˜ê¸°",
            onClick: handleViewMeetingMinutes,
            variant: "default",
          },
        ]}
      />

      <ContentDialog
        isOpen={meetingMinutesState.showContentDialog}
        onClose={handleMeetingMinutesClose}
        characterImage={popoImage}
        title={workspace?.title ? `${workspace.title} íšŒì˜` : "íšŒì˜ë¡"}
        content={
          meetingMinutesState.error
            ? `# âŒ ì˜¤ë¥˜ ë°œìƒ\n\n${meetingMinutesState.error}`
            : meetingMinutesState.content || "# ðŸ“ íšŒì˜ë¡\n\níšŒì˜ë¡ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
        isLoading={meetingMinutesState.isGenerating}
        buttons={[
          {
            id: "copy",
            text: "ë³µì‚¬í•˜ê¸°",
            onClick: () => {
              if (meetingMinutesState.content) {
                navigator.clipboard
                  .writeText(meetingMinutesState.content)
                  .then(() => {
                    alert("íšŒì˜ë¡ì´ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!");
                  })
                  .catch((err) => {
                    console.error("[VoiceChat] Failed to copy:", err);
                    alert("ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.");
                  });
              }
            },
            variant: "outline",
          },
          {
            id: "close",
            text: "ë‹«ê¸°",
            onClick: handleMeetingMinutesClose,
            variant: "default",
          },
        ]}
      />
    </>
  );
};

export default VoiceChat;