"""
LLaVA ë¡œì»¬ ë¹„ì „ ë¶„ì„ ëª¨ë“ˆ
- ì™„ì „ ë¬´ë£Œ, ì˜¤í”„ë¼ì¸ ê°€ëŠ¥
- VRAM: 13GB
"""
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from typing import List, Dict, Optional
from PIL import Image
import os


class LLaVAAnalyzer:
    """LLaVA ë¡œì»¬ ì´ë¯¸ì§€ ë¶„ì„ê¸°"""

    def __init__(
        self,
        model_name: str = "llava-hf/llava-1.5-13b-hf",
        quantization: str = "int4"
    ):
        """
        Args:
            model_name: LLaVA ëª¨ë¸ ì´ë¦„
            quantization: "int4", "int8", "fp16", None
        """
        print(f"ğŸš€ LLaVA ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print(f"ğŸ“Š ì–‘ìí™”: {quantization if quantization else 'FP16'}")

        self.processor = AutoProcessor.from_pretrained(model_name)

        # ì–‘ìí™” ì„¤ì •
        if quantization == "int4" and torch.cuda.is_available():
            from transformers import BitsAndBytesConfig

            print("âš™ï¸  INT4 ì–‘ìí™” ì„¤ì • (VRAM ~7GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            self.model = LlavaForConditionalGeneration.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16
            )
        elif quantization == "int8" and torch.cuda.is_available():
            from transformers import BitsAndBytesConfig

            print("âš™ï¸  INT8 ì–‘ìí™” ì„¤ì • (VRAM ~13GB)")
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            self.model = LlavaForConditionalGeneration.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16
            )
        else:
            print("âš™ï¸  FP16 ì„¤ì • (VRAM ~26GB)")
            self.model = LlavaForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )

        if torch.cuda.is_available():
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\n")
        else:
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (CPU ëª¨ë“œ)\n")

    def analyze_frame(
        self,
        image_path: str,
        prompt: str = None,
        language: str = "ko",
        max_tokens: int = 256
    ) -> Dict:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
            language: ì‘ë‹µ ì–¸ì–´
            max_tokens: ìµœëŒ€ í† í° ìˆ˜

        Returns:
            {'success': bool, 'description': str, 'path': str}
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert('RGB')

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            if not prompt:
                if language == "ko":
                    prompt = """USER: <image>
ì´ ì´ë¯¸ì§€ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ê°ì²´ë‚˜ ì¸ë¬¼
2. ë°°ê²½ì´ë‚˜ ì¥ì†Œ
3. í™”ë©´ì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸
4. ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°

ASSISTANT:"""
                else:
                    prompt = """USER: <image>
Describe this image in detail, including:
1. Main objects or people
2. Background or location
3. Any visible text
4. Overall atmosphere

ASSISTANT:"""

            # í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
            inputs = self.processor(
                text=prompt,
                images=image,
                return_tensors="pt"
            ).to(self.model.device)

            # ìƒì„±
            with torch.no_grad():
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    do_sample=True
                )

            # ë””ì½”ë”©
            description = self.processor.decode(
                output[0],
                skip_special_tokens=True
            )

            # "ASSISTANT:" ì´í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            if "ASSISTANT:" in description:
                description = description.split("ASSISTANT:")[-1].strip()

            return {
                'success': True,
                'description': description,
                'path': image_path
            }

        except Exception as e:
            return {
                'success': False,
                'error': f'LLaVA analysis failed: {str(e)}',
                'path': image_path
            }

    def analyze_frames_batch(
        self,
        frames: List[Dict],
        prompt: str = None,
        language: str = "ko"
    ) -> List[Dict]:
        """
        ì—¬ëŸ¬ í”„ë ˆì„ ë°°ì¹˜ ë¶„ì„

        Args:
            frames: [{'timestamp': float, 'path': str}]
            prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
            language: ì‘ë‹µ ì–¸ì–´

        Returns:
            [{'timestamp': float, 'path': str, 'description': str}]
        """
        results = []

        for idx, frame in enumerate(frames):
            print(f"ğŸ” [{idx+1}/{len(frames)}] ë¶„ì„ ì¤‘...")

            result = self.analyze_frame(
                frame['path'],
                prompt=prompt,
                language=language
            )

            result['timestamp'] = frame.get('timestamp', 0)
            results.append(result)

            if result['success']:
                timestamp = self._format_timestamp(result['timestamp'])
                print(f"âœ… [{timestamp}] ì™„ë£Œ")
            else:
                print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")

        return results

    def analyze_with_context(
        self,
        image_path: str,
        context: str,
        language: str = "ko"
    ) -> Dict:
        """
        ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            language: ì‘ë‹µ ì–¸ì–´
        """
        if language == "ko":
            prompt = f"""USER: <image>
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

{context}

ì´ë¯¸ì§€ì™€ ì»¨í…ìŠ¤íŠ¸ì˜ ì—°ê²°ì„±ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ASSISTANT:"""
        else:
            prompt = f"""USER: <image>
Analyze this image with the following context:

{context}

Explain how the image relates to the context.

ASSISTANT:"""

        return self.analyze_frame(image_path, prompt=prompt)

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """ì´ˆë¥¼ MM:SS í˜•íƒœë¡œ ë³€í™˜"""
        mins = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{mins:02d}:{secs:02d}"


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python llava_analyzer.py <image_path>")
        sys.exit(1)

    image_path = sys.argv[1]

    print("ğŸ¦™ LLaVA ë¶„ì„ê¸° ì´ˆê¸°í™”...")
    analyzer = LLaVAAnalyzer(quantization="int4")

    print("\nğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
    result = analyzer.analyze_frame(image_path, language="ko")

    if result['success']:
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ:\n{result['description']}")
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")