"""
Gemini Vision APIë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“ˆ
"""
import os
import google.generativeai as genai
from typing import List, Dict, Optional
from pathlib import Path
from dotenv import load_dotenv
import base64
from PIL import Image

load_dotenv()


class VisionAnalyzer:
    """Gemini Vision API ì´ë¯¸ì§€ ë¶„ì„ê¸°"""

    def __init__(self, api_key: str = None):
        """
        Args:
            api_key: Gemini API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")

        genai.configure(api_key=self.api_key)
        # gemini-2.0-flash ì‚¬ìš© (ë¹ ë¥´ê³  ë¹„ìš© íš¨ìœ¨ì : $0.10/1M input, $0.40/1M output)
        self.model = genai.GenerativeModel('gemini-2.0-flash')

    def analyze_frame(
        self,
        image_path: str,
        prompt: str = None,
        language: str = "ko"
    ) -> Dict:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ (Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            language: ì‘ë‹µ ì–¸ì–´

        Returns:
            {
                'success': bool,
                'description': str,
                'error': str (optional)
            }
        """
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = Image.open(image_path)

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            if not prompt:
                if language == "ko":
                    prompt = """ì´ ì´ë¯¸ì§€ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ê°ì²´ë‚˜ ì¸ë¬¼
2. ë°°ê²½ì´ë‚˜ ì¥ì†Œ
3. í™”ë©´ì— ë³´ì´ëŠ” í…ìŠ¤íŠ¸ (ìˆë‹¤ë©´)
4. ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°ë‚˜ ìƒí™©

ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
                else:
                    prompt = """Describe this image in detail, including:
1. Main objects or people
2. Background or location
3. Any visible text
4. Overall atmosphere or situation

Be concise and clear."""

            # Geminië¡œ ë¶„ì„
            response = self.model.generate_content([prompt, img])

            return {
                'success': True,
                'description': response.text,
                'path': image_path
            }

        except Exception as e:
            return {
                'success': False,
                'error': f'Vision analysis failed: {str(e)}',
                'path': image_path
            }

    def analyze_frames_batch(
        self,
        frames: List[Dict],
        prompt: str = None,
        language: str = "ko"
    ) -> List[Dict]:
        """
        ì—¬ëŸ¬ í”„ë ˆì„ ë°°ì¹˜ ë¶„ì„

        Args:
            frames: [{'timestamp': float, 'path': str}] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
            prompt: ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
            language: ì‘ë‹µ ì–¸ì–´

        Returns:
            [{'timestamp': float, 'path': str, 'description': str, 'success': bool}]
        """
        results = []

        for frame in frames:
            result = self.analyze_frame(
                frame['path'],
                prompt=prompt,
                language=language
            )

            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            result['timestamp'] = frame.get('timestamp', 0)

            results.append(result)

            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if result['success']:
                timestamp = self._format_timestamp(result['timestamp'])
                print(f"âœ… [{timestamp}] ë¶„ì„ ì™„ë£Œ")
            else:
                print(f"âŒ [{timestamp}] ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'Unknown')}")

        return results

    def analyze_with_context(
        self,
        image_path: str,
        context: str,
        language: str = "ko"
    ) -> Dict:
        """
        ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì˜ˆ: ìë§‰ ë‚´ìš©)
            language: ì‘ë‹µ ì–¸ì–´

        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        if language == "ko":
            prompt = f"""ì´ ì´ë¯¸ì§€ë¥¼ ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ë‚´ìš©ê³¼ ì»¨í…ìŠ¤íŠ¸ê°€ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        else:
            prompt = f"""Analyze this image with the following context.

Context:
{context}

Explain how the image relates to the context."""

        return self.analyze_frame(image_path, prompt=prompt, language=language)

    def extract_text_from_image(self, image_path: str) -> Dict:
        """
        ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)

        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ

        Returns:
            {'success': bool, 'text': str}
        """
        try:
            img = Image.open(image_path)

            prompt = """ì´ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
- í…ìŠ¤íŠ¸ê°€ ì—†ë‹¤ë©´ "í…ìŠ¤íŠ¸ ì—†ìŒ"ì´ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”."""

            response = self.model.generate_content([prompt, img])

            return {
                'success': True,
                'text': response.text,
                'path': image_path
            }

        except Exception as e:
            return {
                'success': False,
                'error': f'OCR failed: {str(e)}',
                'path': image_path
            }

    def summarize_visual_content(
        self,
        analyses: List[Dict],
        video_title: str = ""
    ) -> str:
        """
        ì—¬ëŸ¬ í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½

        Args:
            analyses: analyze_frames_batchì˜ ê²°ê³¼
            video_title: ì˜ìƒ ì œëª©

        Returns:
            ìš”ì•½ëœ ë¹„ì£¼ì–¼ ì½˜í…ì¸  ì„¤ëª…
        """
        summary_parts = []

        if video_title:
            summary_parts.append(f"ì˜ìƒ ì œëª©: {video_title}\n")

        summary_parts.append("=== ì£¼ìš” ì¥ë©´ ë¶„ì„ ===\n")

        for analysis in analyses:
            if analysis['success']:
                timestamp = self._format_timestamp(analysis['timestamp'])
                desc = analysis['description']
                summary_parts.append(f"[{timestamp}] {desc}\n")

        return '\n'.join(summary_parts)

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """ì´ˆë¥¼ MM:SS í˜•íƒœë¡œ ë³€í™˜"""
        mins = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{mins:02d}:{secs:02d}"


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python vision_analyzer.py <image_path>")
        sys.exit(1)

    image_path = sys.argv[1]

    analyzer = VisionAnalyzer()

    print("ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
    result = analyzer.analyze_frame(image_path)

    if result['success']:
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ:\n{result['description']}")
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")

    # OCR í…ŒìŠ¤íŠ¸
    print("\nğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    ocr_result = analyzer.extract_text_from_image(image_path)

    if ocr_result['success']:
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ:\n{ocr_result['text']}")
    else:
        print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {ocr_result['error']}")