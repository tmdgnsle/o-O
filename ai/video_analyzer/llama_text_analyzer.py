"""
Llama 3.1 8B Text Analyzer
í…ìŠ¤íŠ¸ ìš”ì•½ ë° í•©ì„±ì„ ìœ„í•œ Llama 3.1 8B ëª¨ë¸ ë˜í¼
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import List, Dict, Optional
import os
from dotenv import load_dotenv

load_dotenv()


class LlamaTextAnalyzer:
    """Llama 3.1 8Bë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìš”ì•½ê¸°"""

    def __init__(
        self,
        model_name: str = "meta-llama/Llama-3.1-8B-Instruct",
        quantization: Optional[str] = "int4"
    ):
        """
        Llama Text Analyzer ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            quantization: "int4", "int8", "fp16", None
        """
        self.model_name = model_name
        print(f"ğŸš€ Llama Text ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print(f"ğŸ“Š ì–‘ìí™”: {quantization if quantization else 'FP16'}")
        print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=os.getenv("HUGGINGFACE_TOKEN")
        )

        # ì–‘ìí™” ì„¤ì •ì— ë”°ë¥¸ ëª¨ë¸ ë¡œë“œ
        if quantization == "int4" and torch.cuda.is_available():
            print("âš™ï¸  INT4 ì–‘ìí™” ì„¤ì • (VRAM ~4GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        elif quantization == "int8" and torch.cuda.is_available():
            print("âš™ï¸  INT8 ì–‘ìí™” ì„¤ì • (VRAM ~8GB)")
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        else:
            print("âš™ï¸  FP16 ì„¤ì • (VRAM ~16GB)")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )

        # VRAM ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            print(f"ğŸ“Š VRAM ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\n")
        else:
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (CPU ëª¨ë“œ)\n")

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 1024,
        temperature: float = 0.7
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # í† í°í™”
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt"
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True if temperature > 0 else False,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        # ë””ì½”ë”© (ì…ë ¥ ì œê±°í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ)
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return generated_text.strip()

    def summarize_video(
        self,
        frame_analyses: List[str],
        transcript: str,
        max_tokens: int = 2048,
        temperature: float = 0.3
    ) -> str:
        """
        ì˜ìƒ í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ì™€ ìë§‰ì„ ì¢…í•©í•˜ì—¬ ìš”ì•½

        Args:
            frame_analyses: ê° í”„ë ˆì„ì˜ ì‹œê° ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            transcript: ì˜ìƒ ìë§‰/ìŒì„± í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)

        Returns:
            ì¢…í•© ìš”ì•½ ê²°ê³¼
        """
        # í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ í¬ë§¤íŒ…
        frame_summary = "\n\n".join([
            f"[í”„ë ˆì„ {i+1}]\n{analysis}"
            for i, analysis in enumerate(frame_analyses)
        ])

        system_prompt = """ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ìƒì˜ ì£¼ìš” í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ì™€ ìŒì„±/ìë§‰ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬
ì˜ìƒ ì „ì²´ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒì€ YouTube ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

## ì‹œê° ì •ë³´ (ì£¼ìš” í”„ë ˆì„ ë¶„ì„)
{frame_summary}

## ìŒì„±/ìë§‰ ë‚´ìš©
{transcript}

---

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì˜ìƒì„ ì¢…í•© ìš”ì•½í•´ì£¼ì„¸ìš”:

1. ì˜ìƒ ì£¼ì œ ë° ê°œìš”
2. ì£¼ìš” ë‚´ìš© (ë¶ˆë¦¿ í¬ì¸íŠ¸)
3. í•µì‹¬ ë©”ì‹œì§€
4. ì£¼ëª©í•  ë§Œí•œ ì‹œê°ì  ìš”ì†Œ
"""

        return self.generate(
            user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

    def synthesize_analysis(
        self,
        vision_results: List[Dict],
        transcript: str,
        additional_context: Optional[str] = None,
        max_tokens: int = 2048,
        temperature: float = 0.3
    ) -> str:
        """
        ë¹„ì „ ë¶„ì„ ê²°ê³¼ (ê°ì²´ ê°ì§€, ì¥ë©´ ë¶„ë¥˜, OCR ë“±)ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©

        Args:
            vision_results: ë¹„ì „ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: [{"frame": 1, "objects": [...], "scene": "...", "text": "..."}]
            transcript: ì˜ìƒ ìë§‰/ìŒì„± í…ìŠ¤íŠ¸
            additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        # ë¹„ì „ ê²°ê³¼ í¬ë§¤íŒ…
        vision_summary = ""
        for result in vision_results:
            frame_num = result.get("frame", "?")
            vision_summary += f"\n[í”„ë ˆì„ {frame_num}]\n"

            if "objects" in result:
                vision_summary += f"- ê°ì§€ëœ ê°ì²´: {', '.join(result['objects'])}\n"
            if "scene" in result:
                vision_summary += f"- ì¥ë©´ ë¶„ë¥˜: {result['scene']}\n"
            if "text" in result and result["text"]:
                vision_summary += f"- í…ìŠ¤íŠ¸(OCR): {result['text']}\n"
            if "description" in result:
                vision_summary += f"- ì„¤ëª…: {result['description']}\n"

        system_prompt = """ë‹¹ì‹ ì€ ë©€í‹°ëª¨ë‹¬ ì˜ìƒ ë¶„ì„ AIì…ë‹ˆë‹¤.
ì»´í“¨í„° ë¹„ì „ ë¶„ì„ ê²°ê³¼ì™€ ìŒì„±/ìë§‰ì„ ì¢…í•©í•˜ì—¬
ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

## ì»´í“¨í„° ë¹„ì „ ë¶„ì„
{vision_summary}

## ìŒì„±/ìë§‰
{transcript}
"""

        if additional_context:
            user_prompt += f"\n## ì¶”ê°€ ì •ë³´\n{additional_context}\n"

        user_prompt += """
---

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

# ì˜ìƒ ë¶„ì„ ë¦¬í¬íŠ¸

## ìš”ì•½

## ì‹œê°ì  í•˜ì´ë¼ì´íŠ¸

## ì£¼ìš” ë‚´ìš©

## í•µì‹¬ ì¸ì‚¬ì´íŠ¸
"""

        return self.generate(
            user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

    def extract_key_points(
        self,
        text: str,
        max_points: int = 5,
        max_tokens: int = 512,
        temperature: float = 0.3
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            max_points: ìµœëŒ€ í¬ì¸íŠ¸ ê°œìˆ˜
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            í•µì‹¬ í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ {max_points}ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ê° í¬ì¸íŠ¸ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , ë²ˆí˜¸ ì—†ì´ "-"ë¡œ ì‹œì‘í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í•µì‹¬ í¬ì¸íŠ¸:"""

        result = self.generate(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

        # ê²°ê³¼ íŒŒì‹±
        points = []
        for line in result.split("\n"):
            line = line.strip()
            if line.startswith("-") or line.startswith("â€¢"):
                points.append(line.lstrip("-â€¢").strip())
            elif line and len(points) < max_points:
                points.append(line)

        return points[:max_points]

    def get_vram_usage(self) -> Dict[str, float]:
        """í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        if torch.cuda.is_available():
            return {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3
            }
        return {"allocated_gb": 0.0, "reserved_gb": 0.0}

    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def main():
    """í…ŒìŠ¤íŠ¸ ì˜ˆì œ"""
    print("=" * 60)
    print("ğŸ¦™ Llama 3.1 8B Text Analyzer í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # Analyzer ì´ˆê¸°í™”
    analyzer = LlamaTextAnalyzer(quantization="int4")

    # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
    print("\nğŸ“ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    result = analyzer.generate(
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        temperature=0.7
    )
    print(f"\nğŸ¤– ìƒì„± ê²°ê³¼:\n{result}\n")

    # VRAM ì‚¬ìš©ëŸ‰ í™•ì¸
    vram = analyzer.get_vram_usage()
    print(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {vram['allocated_gb']:.2f} GB")

    # í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
    sample_text = """
    ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒì—ì„œ ì ì  ë” ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    ì˜ë£Œ, êµìœ¡, ê¸ˆìœµ, ì œì¡°ì—… ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ AIê°€ í™œìš©ë˜ê³  ìˆìœ¼ë©°,
    íŠ¹íˆ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
    í•˜ì§€ë§Œ AI ìœ¤ë¦¬ì™€ ê°œì¸ì •ë³´ ë³´í˜¸ ë¬¸ì œë„ í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    points = analyzer.extract_key_points(sample_text, max_points=3)
    print("í•µì‹¬ í¬ì¸íŠ¸:")
    for i, point in enumerate(points, 1):
        print(f"{i}. {point}")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    analyzer.cleanup()


if __name__ == "__main__":
    main()
