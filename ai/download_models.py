#!/usr/bin/env python3
"""
Llama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""
import os
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, MllamaForConditionalGeneration

load_dotenv()


def download_llama_3_1():
    """Llama 3.1 8B Instruct ë‹¤ìš´ë¡œë“œ"""
    print("\n" + "="*60)
    print("ğŸ”„ Llama 3.1 8B Instruct ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print("="*60)

    model_name = "meta-llama/Llama-3.1-8B-Instruct"
    token = os.getenv("HUGGINGFACE_TOKEN")

    if not token:
        print("âŒ HUGGINGFACE_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ .env íŒŒì¼ì— HUGGINGFACE_TOKENì„ ì¶”ê°€í•˜ì„¸ìš”.")
        return False

    try:
        print("\n1/2 í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
        print("âœ… í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        print("\n2/2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 15GB, ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(model_name, token=token)
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        print("\nğŸ‰ Llama 3.1 8B Instruct ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return True

    except Exception as e:
        print(f"\nâŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def download_llama_3_2_vision():
    """Llama 3.2 11B Vision Instruct ë‹¤ìš´ë¡œë“œ"""
    print("\n" + "="*60)
    print("ğŸ”„ Llama 3.2 11B Vision Instruct ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print("="*60)

    model_name = "meta-llama/Llama-3.2-11B-Vision-Instruct"
    token = os.getenv("HUGGINGFACE_TOKEN")

    if not token:
        print("âŒ HUGGINGFACE_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ .env íŒŒì¼ì— HUGGINGFACE_TOKENì„ ì¶”ê°€í•˜ì„¸ìš”.")
        return False

    try:
        print("\n1/3 í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
        print("âœ… í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        print("\n2/3 í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        processor = AutoProcessor.from_pretrained(model_name, token=token)
        print("âœ… í”„ë¡œì„¸ì„œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        print("\n3/3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 22GB, ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)")
        model = MllamaForConditionalGeneration.from_pretrained(model_name, token=token)
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        print("\nğŸ‰ Llama 3.2 11B Vision Instruct ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        return True

    except Exception as e:
        print(f"\nâŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸš€ Llama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë„êµ¬")
    print("="*60)
    print("\në‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("  1. Llama 3.1 8B Instruct (~15GB)")
    print("  2. Llama 3.2 11B Vision Instruct (~22GB)")
    print("  3. ë‘˜ ë‹¤ ë‹¤ìš´ë¡œë“œ (~37GB)")
    print("  4. ì¢…ë£Œ")

    choice = input("\nì„ íƒ (1-4): ").strip()

    if choice == "1":
        download_llama_3_1()
    elif choice == "2":
        download_llama_3_2_vision()
    elif choice == "3":
        print("\nğŸ“¦ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ì•½ 37GB)")
        success1 = download_llama_3_1()
        success2 = download_llama_3_2_vision()

        if success1 and success2:
            print("\n" + "="*60)
            print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
            print("="*60)
        else:
            print("\nâš ï¸  ì¼ë¶€ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
    elif choice == "4":
        print("\nâœ… ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        print("\nâŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ë‹¤ìš´ë¡œë“œë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
