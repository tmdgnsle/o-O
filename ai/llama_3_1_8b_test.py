"""
Llama 3.1 8B í…ŒìŠ¤íŠ¸
RTX A6000 48GB VRAM ìµœì í™”
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class Llama31Chat:
    def __init__(self, model_name="meta-llama/Llama-3.1-8B-Instruct", quantization="int4"):
        """
        Llama 3.1 8B ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            model_name: ëª¨ë¸ ì´ë¦„
            quantization: "int4", "int8", "fp16", None
        """
        self.model_name = model_name
        print(f"ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print(f"ğŸ“Š ì–‘ìí™”: {quantization if quantization else 'FP16'}")
        print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=os.getenv("HUGGINGFACE_TOKEN")
        )

        # ì–‘ìí™” ì„¤ì •
        if quantization == "int4" and torch.cuda.is_available():
            print("âš™ï¸  INT4 ì–‘ìí™” ì„¤ì • (VRAM ~4GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        elif quantization == "int8" and torch.cuda.is_available():
            print("âš™ï¸  INT8 ì–‘ìí™” ì„¤ì • (VRAM ~8GB)")
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        else:
            print("âš™ï¸  FP16 ì„¤ì • (VRAM ~16GB)")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )

        self.conversation_history = []

        # VRAM ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            print(f"ğŸ“Š VRAM ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\n")
        else:
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (CPU ëª¨ë“œ)\n")

    def chat(self, user_message, max_tokens=512, temperature=0.7):
        """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_prompt = self.tokenizer.apply_chat_template(
            self.conversation_history,
            tokenize=False,
            add_generation_prompt=True
        )

        # í† í°í™”
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt"
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µë§Œ ì¶”ì¶œ
        if "assistant" in full_response:
            assistant_response = full_response.split("assistant")[-1].strip()
        else:
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            assistant_response = full_response[len(formatted_prompt):].strip()

        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.conversation_history.append({
            "role": "assistant",
            "content": assistant_response
        })

        return assistant_response

    def reset(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ğŸ”„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”\n")

    def get_vram_usage(self):
        """í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            return f"ì‚¬ìš©: {allocated:.2f}GB / ì˜ˆì•½: {reserved:.2f}GB"
        return "CPU ëª¨ë“œ"


def main():
    print("=" * 50)
    print("ğŸ¦™ Llama 3.1 8B ëŒ€í™”í˜• ì±—ë´‡")
    print("=" * 50)
    print("\nëª…ë ¹ì–´:")
    print("  'exit' / 'quit': ì¢…ë£Œ")
    print("  'reset': ëŒ€í™” ì´ˆê¸°í™”")
    print("  'history': ëŒ€í™” íˆìŠ¤í† ë¦¬")
    print("  'vram': VRAM ì‚¬ìš©ëŸ‰\n")

    # ì±—ë´‡ ì´ˆê¸°í™” (INT4 ì–‘ìí™” - 48GB VRAMì— ìµœì )
    chatbot = Llama31Chat(quantization="int4")

    # ëŒ€í™” ë£¨í”„
    while True:
        try:
            user_input = input("You: ").strip()

            if user_input.lower() in ['exit', 'quit']:
                print("\nğŸ‘‹ ì±„íŒ… ì¢…ë£Œ")
                break

            if user_input.lower() == 'reset':
                chatbot.reset()
                continue

            if user_input.lower() == 'vram':
                print(f"ğŸ“Š VRAM: {chatbot.get_vram_usage()}\n")
                continue

            if user_input.lower() == 'history':
                print("\n" + "=" * 50)
                print("ğŸ“œ ëŒ€í™” íˆìŠ¤í† ë¦¬")
                print("=" * 50)
                for msg in chatbot.conversation_history:
                    role = "You" if msg["role"] == "user" else "ğŸ¤–"
                    print(f"{role}: {msg['content']}\n")
                continue

            if not user_input:
                continue

            # ì‘ë‹µ ìƒì„±
            print("ğŸ¤–: ", end="", flush=True)
            response = chatbot.chat(user_input)
            print(f"{response}\n")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì±„íŒ… ì¢…ë£Œ")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}\n")


if __name__ == "__main__":
    main()
