"""
import logging
logger = logging.getLogger(__name__)
YouTube ì˜ìƒ ë¶„ì„ - í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ (CV + LLM)
RTX A5000 24GB VRAM ìµœì í™”

íŒŒì´í”„ë¼ì¸:
1. ì˜ìƒ ë‹¤ìš´ë¡œë“œ & í”„ë ˆì„ ì¶”ì¶œ
2. ìë§‰ ì¶”ì¶œ
3. CV ê¸°ë°˜ êµ¬ì¡°í™” ë¶„ì„ (YOLO + CLIP + OCR)
4. Llama 3.1 í…ìŠ¤íŠ¸ LLMìœ¼ë¡œ ì¢…í•© ìš”ì•½
"""
import logging

logger = logging.getLogger(__name__)

import os
from dotenv import load_dotenv
from video_analyzer.frame_extractor import FrameExtractor
from video_analyzer.transcript_extractor import TranscriptExtractor
import tempfile
import shutil
import torch
from PIL import Image
import numpy as np

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class HybridVisionAnalyzer:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¹„ì „ ë¶„ì„ê¸°
    - CV ëª¨ë¸: YOLO + CLIP + PaddleOCR
    - LLM: Llama 3.1 8B (í…ìŠ¤íŠ¸ ì „ìš©)
    """

    def __init__(
        self,
        use_yolo: bool = True,
        use_clip: bool = True,
        use_ocr: bool = True,
        use_llm: bool = True,
        llm_quantization: str = "int4"
    ):
        """
        Args:
            use_yolo: YOLO ê°ì²´ ê°ì§€ ì‚¬ìš© ì—¬ë¶€
            use_clip: CLIP ì¥ë©´ ë¶„ë¥˜ ì‚¬ìš© ì—¬ë¶€
            use_ocr: OCR ì‚¬ìš© ì—¬ë¶€
            use_llm: Llama í…ìŠ¤íŠ¸ LLM ì‚¬ìš© ì—¬ë¶€
            llm_quantization: LLM ì–‘ìí™” ("int4", "int8", "fp16")
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"ğŸ–¥ï¸  ë””ë°”ì´ìŠ¤: {self.device}")

        self.yolo_model = None
        self.clip_model = None
        self.ocr_model = None
        self.llm_model = None

        # ê° ëª¨ë¸ ì´ˆê¸°í™”
        if use_yolo:
            self._load_yolo()

        if use_clip:
            self._load_clip()

        if use_ocr:
            self._load_ocr()

        if use_llm:
            self._load_llm(llm_quantization)

    def _load_yolo(self):
        """YOLO v11 nano ë¡œë“œ (ê°ì²´ ê°ì§€)"""
        logger.info("\nğŸ” YOLO v11 nano ë¡œë”© ì¤‘...")
        try:
            from ultralytics import YOLO
            # nano ë²„ì „ ì‚¬ìš© (ê°€ì¥ ê°€ë²¼ì›€)
            self.yolo_model = YOLO('yolo11n.pt')
            logger.info("âœ… YOLO v11 nano ë¡œë“œ ì™„ë£Œ (VRAM ~1GB)")
        except Exception as e:
            logger.info(f"âš ï¸  YOLO ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("   ì„¤ì¹˜: pip install ultralytics")

    def _load_clip(self):
        """CLIP ë¡œë“œ (ì¥ë©´ ë¶„ë¥˜)"""
        logger.info("\nğŸ¨ CLIP ë¡œë”© ì¤‘...")
        try:
            import clip
            self.clip_model, self.clip_preprocess = clip.load(
                "ViT-B/32",
                device=self.device
            )
            logger.info("âœ… CLIP ViT-B/32 ë¡œë“œ ì™„ë£Œ (VRAM ~1GB)")
        except Exception as e:
            logger.info(f"âš ï¸  CLIP ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("   ì„¤ì¹˜: pip install git+https://github.com/openai/CLIP.git")

    def _load_ocr(self):
        """PaddleOCR ë¡œë“œ (í•œê¸€ OCR)"""
        logger.info("\nğŸ“ PaddleOCR ë¡œë”© ì¤‘...")
        try:
            from paddleocr import PaddleOCR
            # í•œê¸€ + ì˜ì–´ ì§€ì›
            self.ocr_model = PaddleOCR(
                lang='korean',
                use_angle_cls=True,
                show_log=False
            )
            logger.info("âœ… PaddleOCR ë¡œë“œ ì™„ë£Œ (VRAM ~500MB)")
        except Exception as e:
            logger.info(f"âš ï¸  OCR ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("   ì„¤ì¹˜: pip install paddleocr paddlepaddle-gpu")

    def _load_llm(self, quantization: str = "int4"):
        """Llama 3.1 8B í…ìŠ¤íŠ¸ LLM ë¡œë“œ"""
        logger.info(f"\nğŸ¦™ Llama 3.1 8B ë¡œë”© ì¤‘ (ì–‘ìí™”: {quantization})...")
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

            model_name = "meta-llama/Llama-3.1-8B-Instruct"

            self.llm_tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                token=os.getenv("HUGGINGFACE_TOKEN")
            )

            # ì–‘ìí™” ì„¤ì •
            if quantization == "int4":
                logger.info("âš™ï¸  INT4 ì–‘ìí™” (VRAM ~4-5GB)")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    token=os.getenv("HUGGINGFACE_TOKEN")
                )
            elif quantization == "int8":
                logger.info("âš™ï¸  INT8 ì–‘ìí™” (VRAM ~8-9GB)")
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    token=os.getenv("HUGGINGFACE_TOKEN")
                )
            else:
                logger.info("âš™ï¸  FP16 (VRAM ~16GB)")
                self.llm_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    token=os.getenv("HUGGINGFACE_TOKEN")
                )

            logger.info("âœ… Llama 3.1 8B ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.info(f"âš ï¸  Llama ë¡œë“œ ì‹¤íŒ¨: {e}")

    def detect_objects(self, image_path: str):
        """YOLOë¡œ ê°ì²´ ê°ì§€"""
        if not self.yolo_model:
            return []

        try:
            results = self.yolo_model(image_path, verbose=False)

            objects = []
            for r in results:
                boxes = r.boxes
                for box in boxes:
                    cls = int(box.cls[0])
                    conf = float(box.conf[0])
                    name = self.yolo_model.names[cls]
                    objects.append({
                        'name': name,
                        'confidence': conf,
                        'bbox': box.xyxy[0].tolist()
                    })

            return objects
        except Exception as e:
            logger.info(f"âš ï¸  YOLO ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def classify_scene(self, image_path: str, candidate_scenes: list):
        """CLIPìœ¼ë¡œ ì¥ë©´ ë¶„ë¥˜ (Zero-shot)"""
        if not self.clip_model:
            return None

        try:
            import clip

            image = self.clip_preprocess(Image.open(image_path)).unsqueeze(0).to(self.device)
            text = clip.tokenize(candidate_scenes).to(self.device)

            with torch.no_grad():
                image_features = self.clip_model.encode_image(image)
                text_features = self.clip_model.encode_text(text)

                similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                values, indices = similarity[0].topk(3)

            # ìƒìœ„ 3ê°œ ë°˜í™˜
            results = []
            for i in range(3):
                results.append({
                    'scene': candidate_scenes[indices[i]],
                    'confidence': float(values[i])
                })

            return results
        except Exception as e:
            logger.info(f"âš ï¸  CLIP ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def extract_text_ocr(self, image_path: str):
        """OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not self.ocr_model:
            return []

        try:
            result = self.ocr_model.ocr(image_path, cls=True)

            texts = []
            if result and result[0]:
                for line in result[0]:
                    text = line[1][0]
                    confidence = line[1][1]
                    texts.append({
                        'text': text,
                        'confidence': confidence
                    })

            return texts
        except Exception as e:
            logger.info(f"âš ï¸  OCR ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def analyze_frame_cv(self, image_path: str, frame_time: float):
        """CV ëª¨ë¸ë¡œ í”„ë ˆì„ êµ¬ì¡°í™” ë¶„ì„"""
        logger.info(f"   ğŸ” CV ë¶„ì„ ì¤‘...")

        result = {
            'time': frame_time,
            'objects': [],
            'scene': None,
            'text': []
        }

        # 1. ê°ì²´ ê°ì§€
        if self.yolo_model:
            result['objects'] = self.detect_objects(image_path)

        # 2. ì¥ë©´ ë¶„ë¥˜
        if self.clip_model:
            scenes = [
                "a news broadcast studio",
                "a government meeting or conference",
                "an interview or discussion",
                "a presentation or lecture",
                "an outdoor scene",
                "a casual conversation",
                "a formal ceremony"
            ]
            result['scene'] = self.classify_scene(image_path, scenes)

        # 3. OCR
        if self.ocr_model:
            result['text'] = self.extract_text_ocr(image_path)

        return result

    def summarize_with_llm(self, cv_results: list, transcript: dict, video_info: dict):
        """Llama 3.1ë¡œ CV ë¶„ì„ ê²°ê³¼ ì¢…í•© ìš”ì•½"""
        if not self.llm_model:
            return "LLM ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        logger.info("\nğŸ¦™ Llama 3.1ë¡œ ì¢…í•© ë¶„ì„ ì¤‘...")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒì€ YouTube ì˜ìƒ "{video_info['title']}"ì˜ AI ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.

ğŸ“Š ì˜ìƒ ì •ë³´:
- ì œëª©: {video_info['title']}
- ì±„ë„: {video_info['channel']}
- ê¸¸ì´: {video_info['duration']}ì´ˆ

"""

        # ìë§‰ ì¶”ê°€
        if transcript and transcript.get('success'):
            prompt += f"""ğŸ“ ìë§‰ ì „ì²´ ë‚´ìš©:
{transcript['full_text'][:500]}...

"""

        # í”„ë ˆì„ë³„ CV ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        prompt += "ğŸ–¼ï¸ í”„ë ˆì„ë³„ ì‹œê°ì  ë¶„ì„:\n\n"

        for i, frame_result in enumerate(cv_results, 1):
            time = frame_result['time']
            timestamp = f"{int(time // 60):02d}:{int(time % 60):02d}"

            prompt += f"[{timestamp}]\n"

            # ì¥ë©´ ë¶„ë¥˜
            if frame_result['scene']:
                top_scene = frame_result['scene'][0]
                prompt += f"- ì¥ë©´ ìœ í˜•: {top_scene['scene']} ({top_scene['confidence']:.1%})\n"

            # ê°ì²´
            if frame_result['objects']:
                obj_names = [obj['name'] for obj in frame_result['objects'][:5]]
                prompt += f"- ê°ì§€ëœ ê°ì²´: {', '.join(obj_names)}\n"

            # OCR í…ìŠ¤íŠ¸
            if frame_result['text']:
                ocr_texts = [t['text'] for t in frame_result['text'][:3]]
                prompt += f"- í™”ë©´ ë‚´ í…ìŠ¤íŠ¸: {' / '.join(ocr_texts)}\n"

            prompt += "\n"

        # ì§ˆë¬¸
        prompt += """ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

1. ì´ ì˜ìƒì˜ ì£¼ì œì™€ í•µì‹¬ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?
2. ì£¼ìš” ë“±ì¥ ì¸ë¬¼ì´ë‚˜ ê°ì²´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
3. ì˜ìƒì˜ ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°ì™€ ë§¥ë½ì€ ì–´ë–¤ê°€ìš”?
4. í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ê°„ê²°í•˜ê³  êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        # LLM ì¶”ë¡ 
        try:
            messages = [
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]

            inputs = self.llm_tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(self.llm_model.device)

            outputs = self.llm_model.generate(
                inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )

            response = self.llm_tokenizer.decode(
                outputs[0][inputs.shape[1]:],
                skip_special_tokens=True
            )

            return response
        except Exception as e:
            logger.info(f"âš ï¸  LLM ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return f"LLM ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def test_hybrid_analysis(
    youtube_url: str,
    max_frames: int = 5,
    llm_quantization: str = "int4"
):
    """
    í•˜ì´ë¸Œë¦¬ë“œ YouTube ì˜ìƒ ë¶„ì„

    Args:
        youtube_url: YouTube URL
        max_frames: ë¶„ì„í•  ìµœëŒ€ í”„ë ˆì„ ìˆ˜
        llm_quantization: LLM ì–‘ìí™” ("int4", "int8", "fp16")
    """
    logger.info("=" * 80)
    logger.info("ğŸ¬ í•˜ì´ë¸Œë¦¬ë“œ YouTube ì˜ìƒ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 80)
    logger.info(f"ğŸ“º URL: {youtube_url}")
    logger.info(f"ğŸ–¼ï¸  ìµœëŒ€ í”„ë ˆì„: {max_frames}ê°œ")
    logger.info(f"ğŸ¦™ LLM ì–‘ìí™”: {llm_quantization}")
    logger.info()

    # Step 1: í”„ë ˆì„ ì¶”ì¶œ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“¥ STEP 1: ì˜ìƒ ë‹¤ìš´ë¡œë“œ & í”„ë ˆì„ ì¶”ì¶œ")
    logger.info("=" * 80)

    temp_dir = tempfile.mkdtemp(prefix="youtube_hybrid_")
    logger.info(f"ğŸ“ ì„ì‹œ ë””ë ‰í† ë¦¬: {temp_dir}")

    extractor = FrameExtractor(output_dir=temp_dir)

    logger.info("â¬ ì˜ìƒ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    video_info = extractor.download_video(youtube_url)

    if not video_info['success']:
        logger.info(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {video_info['error']}")
        return

    logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    logger.info(f"   ì œëª©: {video_info['title']}")
    logger.info(f"   ì±„ë„: {video_info['channel']}")
    logger.info(f"   ê¸¸ì´: {video_info['duration']}ì´ˆ")

    logger.info("\nğŸï¸  í”„ë ˆì„ ì¶”ì¶œ ì¤‘...")
    frame_result = extractor.extract_frames_scene_detect(
        video_path=video_info['path'],
        max_frames=max_frames
    )

    if not frame_result['success']:
        logger.info(f"âŒ í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {frame_result.get('error', 'Unknown')}")
        return

    frames = frame_result['frames']
    logger.info(f"âœ… {len(frames)}ê°œ í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ!")
    for i, frame in enumerate(frames, 1):
        timestamp_str = f"{int(frame['timestamp'] // 60):02d}:{int(frame['timestamp'] % 60):02d}"
        logger.info(f"   {i}. {timestamp_str} - {frame['path']}")

    # Step 2: ìë§‰ ì¶”ì¶œ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ STEP 2: ìë§‰ ì¶”ì¶œ")
    logger.info("=" * 80)

    transcript_result = TranscriptExtractor.get_transcript(
        url=youtube_url,
        languages=['ko', 'en']
    )

    if transcript_result['success']:
        logger.info(f"âœ… ìë§‰ ì¶”ì¶œ ì™„ë£Œ!")
        logger.info(f"   ì–¸ì–´: {transcript_result['language']}")
        logger.info(f"   ì„¸ê·¸ë¨¼íŠ¸: {len(transcript_result['segments'])}ê°œ")
        logger.info(f"\nğŸ“„ ìë§‰ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 300ì):")
        logger.info("-" * 80)
        logger.info(transcript_result['full_text'][:300] + "...")
        logger.info("-" * 80)
    else:
        logger.info(f"âš ï¸  ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {transcript_result.get('error', 'Unknown')}")

    # Step 3: í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¤– STEP 3: í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™”")
    logger.info("=" * 80)

    analyzer = HybridVisionAnalyzer(
        use_yolo=True,
        use_clip=True,
        use_ocr=True,
        use_llm=True,
        llm_quantization=llm_quantization
    )

    # Step 4: CV ê¸°ë°˜ í”„ë ˆì„ ë¶„ì„
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ” STEP 4: CV ê¸°ë°˜ í”„ë ˆì„ ë¶„ì„")
    logger.info("=" * 80)

    cv_results = []
    for i, frame in enumerate(frames, 1):
        timestamp_str = f"{int(frame['timestamp'] // 60):02d}:{int(frame['timestamp'] % 60):02d}"
        logger.info(f"\nğŸ–¼ï¸  í”„ë ˆì„ {i}/{len(frames)} ({timestamp_str})")

        result = analyzer.analyze_frame_cv(frame['path'], frame['timestamp'])
        cv_results.append(result)

        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        if result['scene']:
            logger.info(f"   ì¥ë©´: {result['scene'][0]['scene']} ({result['scene'][0]['confidence']:.1%})")
        if result['objects']:
            obj_names = [obj['name'] for obj in result['objects'][:5]]
            logger.info(f"   ê°ì²´: {', '.join(obj_names)}")
        if result['text']:
            ocr_texts = [t['text'] for t in result['text'][:2]]
            logger.info(f"   í…ìŠ¤íŠ¸: {' / '.join(ocr_texts)}")

    # Step 5: LLM ì¢…í•© ë¶„ì„
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ§  STEP 5: LLM ì¢…í•© ë¶„ì„ ë° ìš”ì•½")
    logger.info("=" * 80)

    summary = analyzer.summarize_with_llm(cv_results, transcript_result, video_info)

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š ìµœì¢… ë¶„ì„ ê²°ê³¼")
    logger.info("=" * 80)
    logger.info()
    logger.info(summary)
    logger.info()

    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    logger.info("\nğŸ—‘ï¸  ì„ì‹œ íŒŒì¼ ì‚­ì œ ì¤‘...")
    try:
        shutil.rmtree(temp_dir)
        logger.info(f"âœ… ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ: {temp_dir}")
    except Exception as e:
        logger.info(f"âš ï¸  ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")

    logger.info("\n" + "=" * 80)
    logger.info("âœ… í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì™„ë£Œ!")
    logger.info("=" * 80)


if __name__ == "__main__":
    import sys

    logger.info("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              YouTube í•˜ì´ë¸Œë¦¬ë“œ ì˜ìƒ ë¶„ì„ (CV + LLM)                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

íŒŒì´í”„ë¼ì¸:
1. ì˜ìƒ ë‹¤ìš´ë¡œë“œ & í”„ë ˆì„ ì¶”ì¶œ
2. ìë§‰ ì¶”ì¶œ
3. CV ë¶„ì„ (YOLO + CLIP + OCR)
4. Llama 3.1 í…ìŠ¤íŠ¸ LLM ì¢…í•© ìš”ì•½

ì‚¬ìš©ë²•:
    python test_youtube_analyzer_hybrid.py <YouTube_URL> [ì˜µì…˜]

ì˜µì…˜:
    --frames N       : ë¶„ì„í•  ìµœëŒ€ í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ê°’: 5)
    --quantization Q : LLM ì–‘ìí™” (int4/int8/fp16, ê¸°ë³¸ê°’: int4)

ì˜ˆì‹œ:
    python test_youtube_analyzer_hybrid.py "https://www.youtube.com/watch?v=VIDEO_ID"
    python test_youtube_analyzer_hybrid.py "https://www.youtube.com/watch?v=VIDEO_ID" --frames 3 --quantization int8

í•„ìˆ˜ ì„¤ì¹˜:
    pip install ultralytics paddleocr paddlepaddle-gpu
    pip install git+https://github.com/openai/CLIP.git

í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ (.env):
    HUGGINGFACE_TOKEN=your_token_here
    """)

    if len(sys.argv) < 2:
        logger.info("\nâŒ YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        logger.info("ì˜ˆì‹œ: python test_youtube_analyzer_hybrid.py 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'\n")
        sys.exit(1)

    # ì¸ì íŒŒì‹±
    youtube_url = sys.argv[1]

    max_frames = 5
    if "--frames" in sys.argv:
        idx = sys.argv.index("--frames")
        if idx + 1 < len(sys.argv):
            max_frames = int(sys.argv[idx + 1])

    llm_quantization = "int4"
    if "--quantization" in sys.argv:
        idx = sys.argv.index("--quantization")
        if idx + 1 < len(sys.argv):
            llm_quantization = sys.argv[idx + 1]

    # ì‹¤í–‰
    test_hybrid_analysis(
        youtube_url=youtube_url,
        max_frames=max_frames,
        llm_quantization=llm_quantization
    )