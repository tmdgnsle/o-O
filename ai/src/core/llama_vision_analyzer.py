"""
Llama 3.2 11B Vision Analyzer
ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•œ Llama 3.2 Vision ëª¨ë¸ ë˜í¼
"""
import torch
import logging
from transformers import MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
from PIL import Image
from typing import List, Dict, Union, Optional
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)


class LlamaVisionAnalyzer:
    """Llama 3.2 11B Visionì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ì„ê¸°"""

    def __init__(
        self,
        model_name: str = "meta-llama/Llama-3.2-11B-Vision-Instruct",
        quantization: Optional[str] = "int4"
    ):
        """
        Llama Vision Analyzer ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            quantization: "int4", "int8", "fp16", None
        """
        self.model_name = model_name
        logger.info(f"ğŸš€ Llama Vision ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        logger.info(f"ğŸ“Š ì–‘ìí™”: {quantization if quantization else 'BF16'}")
        logger.info(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n")

        # Processor (í† í¬ë‚˜ì´ì € + ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ) ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(
            model_name,
            token=os.getenv("HUGGINGFACE_TOKEN")
        )

        # ì–‘ìí™” ì„¤ì •ì— ë”°ë¥¸ ëª¨ë¸ ë¡œë“œ
        if quantization == "int4" and torch.cuda.is_available():
            logger.info("âš™ï¸  INT4 ì–‘ìí™” ì„¤ì • (VRAM ~10GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                llm_int8_enable_fp32_cpu_offload=True
            )
            self.model = MllamaForConditionalGeneration.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        elif quantization == "int8" and torch.cuda.is_available():
            logger.info("âš™ï¸  INT8 ì–‘ìí™” ì„¤ì • (VRAM ~15GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True
            )
            self.model = MllamaForConditionalGeneration.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        else:
            logger.info("âš™ï¸  BF16 ì„¤ì • (VRAM ~22GB)")
            self.model = MllamaForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )

        # VRAM ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            logger.info(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            logger.info(f"ğŸ“Š VRAM ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\n")
        else:
            logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (CPU ëª¨ë“œ)\n")

    def analyze_image(
        self,
        image: Union[str, Image.Image],
        prompt: str = "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        max_tokens: int = 1024,
        temperature: float = 0.7,
        repetition_penalty: float = 1.0
    ) -> str:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            image: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” PIL Image ê°ì²´
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            repetition_penalty: ë°˜ë³µ ì–µì œ (1.0=ì—†ìŒ, 1.2-1.5 ê¶Œì¥)

        Returns:
            ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            pil_image = Image.open(image).convert("RGB")
        else:
            pil_image = image.convert("RGB")

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": prompt}
                ]
            }
        ]

        # ì…ë ¥ ì¤€ë¹„
        input_text = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True
        )

        inputs = self.processor(
            images=pil_image,
            text=input_text,
            return_tensors="pt"
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True if temperature > 0 else False,
                pad_token_id=self.processor.tokenizer.pad_token_id,
                repetition_penalty=repetition_penalty
            )

        # ë””ì½”ë”© (ì…ë ¥ ì œê±°í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ)
        generated_text = self.processor.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return generated_text.strip()

    def analyze_images_batch(
        self,
        images: List[Union[str, Image.Image]],
        prompt: str = "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        max_tokens: int = 1024,
        temperature: float = 0.7,
        repetition_penalty: float = 1.0,
        batch_size: int = 1
    ) -> List[str]:
        """
        ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ë°°ì¹˜ë¡œ ë¶„ì„

        ë°°ì¹˜ í¬ê¸°ê°€ 1ë³´ë‹¤ í¬ë©´ ë™ì‹œì— ì—¬ëŸ¬ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ì§€ë§Œ,
        VRAM ì œì•½ìœ¼ë¡œ ì¸í•´ ê¸°ë³¸ê°’ì€ 1ì…ë‹ˆë‹¤.

        Args:
            images: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” PIL Image ê°ì²´ ë¦¬ìŠ¤íŠ¸
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            repetition_penalty: ë°˜ë³µ ì–µì œ (1.0=ì—†ìŒ, 1.2-1.5 ê¶Œì¥)
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 1, ìˆœì°¨ ì²˜ë¦¬)

        Returns:
            ê° ì´ë¯¸ì§€ì˜ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for batch_start in range(0, len(images), batch_size):
            batch_end = min(batch_start + batch_size, len(images))
            batch_images = images[batch_start:batch_end]

            logger.info(f"ğŸ“¸ ì´ë¯¸ì§€ ë°°ì¹˜ {batch_start+1}-{batch_end}/{len(images)} ë¶„ì„ ì¤‘...")

            # ë°°ì¹˜ ë‚´ ì´ë¯¸ì§€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (í˜„ì¬ ëª¨ë¸ì´ ë°°ì¹˜ ì¶”ë¡ ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ)
            for i, image in enumerate(batch_images):
                result = self.analyze_image(image, prompt, max_tokens, temperature, repetition_penalty)
                results.append(result)

        return results

    def analyze_with_context(
        self,
        image: Union[str, Image.Image],
        prompt: str,
        context: str,
        max_tokens: int = 1024,
        temperature: float = 0.7,
        repetition_penalty: float = 1.0
    ) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì´ë¯¸ì§€ ë¶„ì„

        Args:
            image: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” PIL Image ê°ì²´
            prompt: ë¶„ì„ í”„ë¡¬í”„íŠ¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì˜ˆ: ì˜ìƒ ìë§‰, ì´ì „ í”„ë ˆì„ ë¶„ì„ ê²°ê³¼)
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            repetition_penalty: ë°˜ë³µ ì–µì œ (1.0=ì—†ìŒ, 1.2-1.5 ê¶Œì¥)

        Returns:
            ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            pil_image = Image.open(image).convert("RGB")
        else:
            pil_image = image.convert("RGB")

        # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"ì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {prompt}"

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": full_prompt}
                ]
            }
        ]

        # ì…ë ¥ ì¤€ë¹„
        input_text = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True
        )

        inputs = self.processor(
            images=pil_image,
            text=input_text,
            return_tensors="pt"
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True if temperature > 0 else False,
                pad_token_id=self.processor.tokenizer.pad_token_id,
                repetition_penalty=repetition_penalty
            )

        # ë””ì½”ë”©
        generated_text = self.processor.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return generated_text.strip()

    def get_vram_usage(self) -> Dict[str, float]:
        """í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        if torch.cuda.is_available():
            return {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3
            }
        return {"allocated_gb": 0.0, "reserved_gb": 0.0}

    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'processor'):
            del self.processor

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def main():
    """í…ŒìŠ¤íŠ¸ ì˜ˆì œ"""
    logger.info("=" * 60)
    logger.info("ğŸ¦™ Llama 3.2 11B Vision Analyzer í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)

    # Analyzer ì´ˆê¸°í™”
    analyzer = LlamaVisionAnalyzer(quantization="int4")

    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    test_image = "test_image.jpg"

    if os.path.exists(test_image):
        # ê¸°ë³¸ ë¶„ì„
        logger.info("\nğŸ“¸ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
        result = analyzer.analyze_image(
            test_image,
            prompt="ì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”? ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        )
        logger.info(f"\nğŸ¤– ë¶„ì„ ê²°ê³¼:\n{result}\n")

        # VRAM ì‚¬ìš©ëŸ‰ í™•ì¸
        vram = analyzer.get_vram_usage()
        logger.info(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {vram['allocated_gb']:.2f} GB")
    else:
        logger.info(f"\nâš ï¸  í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ '{test_image}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info("ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    analyzer.cleanup()


if __name__ == "__main__":
    main()
