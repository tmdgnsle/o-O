"""
Llama 3.1 8B Text Analyzer
í…ìŠ¤íŠ¸ ìš”ì•½ ë° í•©ì„±ì„ ìœ„í•œ Llama 3.1 8B ëª¨ë¸ ë˜í¼
"""
import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import List, Dict, Optional
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)


class LlamaTextAnalyzer:
    """Llama 3.1 8Bë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìš”ì•½ê¸°"""

    def __init__(
        self,
        model_name: str = "meta-llama/Llama-3.1-8B-Instruct",
        quantization: Optional[str] = "int4"
    ):
        """
        Llama Text Analyzer ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            quantization: "int4", "int8", "fp16", None
        """
        self.model_name = model_name
        logger.info(f"ğŸš€ Llama Text ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        logger.info(f"ğŸ“Š ì–‘ìí™”: {quantization if quantization else 'FP16'}")
        logger.info(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=os.getenv("HUGGINGFACE_TOKEN")
        )

        # ì–‘ìí™” ì„¤ì •ì— ë”°ë¥¸ ëª¨ë¸ ë¡œë“œ
        if quantization == "int4" and torch.cuda.is_available():
            logger.info("âš™ï¸  INT4 ì–‘ìí™” ì„¤ì • (VRAM ~4GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        elif quantization == "int8" and torch.cuda.is_available():
            logger.info("âš™ï¸  INT8 ì–‘ìí™” ì„¤ì • (VRAM ~8GB)")
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        else:
            logger.info("âš™ï¸  FP16 ì„¤ì • (VRAM ~16GB)")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )

        # VRAM ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            logger.info(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
            logger.info(f"ğŸ“Š VRAM ì˜ˆì•½: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\n")
        else:
            logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (CPU ëª¨ë“œ)\n")

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 1024,
        temperature: float = 0.7
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # í† í°í™”
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt"
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                do_sample=True if temperature > 0 else False,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        # ë””ì½”ë”© (ì…ë ¥ ì œê±°í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ)
        generated_text = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return generated_text.strip()

    def summarize_video(
        self,
        frame_analyses: List[str],
        transcript: str,
        max_tokens: int = 2048,
        temperature: float = 0.3
    ) -> str:
        """
        ì˜ìƒ í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ì™€ ìë§‰ì„ ì¢…í•©í•˜ì—¬ ìš”ì•½

        Args:
            frame_analyses: ê° í”„ë ˆì„ì˜ ì‹œê° ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            transcript: ì˜ìƒ ìë§‰/ìŒì„± í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ)

        Returns:
            ì¢…í•© ìš”ì•½ ê²°ê³¼
        """
        # í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ í¬ë§¤íŒ…
        frame_summary = "\n\n".join([
            f"[í”„ë ˆì„ {i+1}]\n{analysis}"
            for i, analysis in enumerate(frame_analyses)
        ])

        system_prompt = """ë‹¹ì‹ ì€ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ìƒì˜ ì£¼ìš” í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ì™€ ìŒì„±/ìë§‰ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬
ì˜ìƒ ì „ì²´ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒì€ YouTube ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

## ì‹œê° ì •ë³´ (ì£¼ìš” í”„ë ˆì„ ë¶„ì„)
{frame_summary}

## ìŒì„±/ìë§‰ ë‚´ìš©
{transcript}

---

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì˜ìƒì„ ì¢…í•© ìš”ì•½í•´ì£¼ì„¸ìš”:

1. ì˜ìƒ ì£¼ì œ ë° ê°œìš”
2. ì£¼ìš” ë‚´ìš© (ë¶ˆë¦¿ í¬ì¸íŠ¸)
3. í•µì‹¬ ë©”ì‹œì§€
4. ì£¼ëª©í•  ë§Œí•œ ì‹œê°ì  ìš”ì†Œ
"""

        return self.generate(
            user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

    def synthesize_analysis(
        self,
        vision_results: List[Dict],
        transcript: str,
        additional_context: Optional[str] = None,
        max_tokens: int = 2048,
        temperature: float = 0.3
    ) -> str:
        """
        ë¹„ì „ ë¶„ì„ ê²°ê³¼ (ê°ì²´ ê°ì§€, ì¥ë©´ ë¶„ë¥˜, OCR ë“±)ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©

        Args:
            vision_results: ë¹„ì „ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
                ì˜ˆ: [{"frame": 1, "objects": [...], "scene": "...", "text": "..."}]
            transcript: ì˜ìƒ ìë§‰/ìŒì„± í…ìŠ¤íŠ¸
            additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        # ë¹„ì „ ê²°ê³¼ í¬ë§¤íŒ…
        vision_summary = ""
        for result in vision_results:
            frame_num = result.get("frame", "?")
            vision_summary += f"\n[í”„ë ˆì„ {frame_num}]\n"

            if "objects" in result:
                vision_summary += f"- ê°ì§€ëœ ê°ì²´: {', '.join(result['objects'])}\n"
            if "scene" in result:
                vision_summary += f"- ì¥ë©´ ë¶„ë¥˜: {result['scene']}\n"
            if "text" in result and result["text"]:
                vision_summary += f"- í…ìŠ¤íŠ¸(OCR): {result['text']}\n"
            if "description" in result:
                vision_summary += f"- ì„¤ëª…: {result['description']}\n"

        system_prompt = """ë‹¹ì‹ ì€ ë©€í‹°ëª¨ë‹¬ ì˜ìƒ ë¶„ì„ AIì…ë‹ˆë‹¤.
ì»´í“¨í„° ë¹„ì „ ë¶„ì„ ê²°ê³¼ì™€ ìŒì„±/ìë§‰ì„ ì¢…í•©í•˜ì—¬
ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”."""

        user_prompt = f"""ë‹¤ìŒì€ ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

## ì»´í“¨í„° ë¹„ì „ ë¶„ì„
{vision_summary}

## ìŒì„±/ìë§‰
{transcript}
"""

        if additional_context:
            user_prompt += f"\n## ì¶”ê°€ ì •ë³´\n{additional_context}\n"

        user_prompt += """
---

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ì˜ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

# ì˜ìƒ ë¶„ì„ ë¦¬í¬íŠ¸

## ìš”ì•½

## ì‹œê°ì  í•˜ì´ë¼ì´íŠ¸

## ì£¼ìš” ë‚´ìš©

## í•µì‹¬ ì¸ì‚¬ì´íŠ¸
"""

        return self.generate(
            user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

    def extract_key_points(
        self,
        text: str,
        max_points: int = 5,
        max_tokens: int = 512,
        temperature: float = 0.3
    ) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            max_points: ìµœëŒ€ í¬ì¸íŠ¸ ê°œìˆ˜
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            í•µì‹¬ í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ {max_points}ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
ê° í¬ì¸íŠ¸ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , ë²ˆí˜¸ ì—†ì´ "-"ë¡œ ì‹œì‘í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í•µì‹¬ í¬ì¸íŠ¸:"""

        result = self.generate(
            prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

        # ê²°ê³¼ íŒŒì‹±
        points = []
        for line in result.split("\n"):
            line = line.strip()
            if line.startswith("-") or line.startswith("â€¢"):
                points.append(line.lstrip("-â€¢").strip())
            elif line and len(points) < max_points:
                points.append(line)

        return points[:max_points]

    def generate_mindmap(
        self,
        video_title: str,
        frame_analyses: List[str],
        transcript: str,
        user_query: Optional[str] = None,
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> str:
        """
        ì˜ìƒ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë§ˆì¸ë“œë§µ êµ¬ì¡°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±

        Args:
            video_title: ì˜ìƒ ì œëª©
            frame_analyses: í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            transcript: ì˜ìƒ ìë§‰
            user_query: ì‚¬ìš©ì ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸ (ì˜ˆ: "ì´ ê¸°ìˆ ë¡œ ì–´ë–¤ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œ?")
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„

        Returns:
            ë§ˆì¸ë“œë§µ JSON ë¬¸ìì—´
        """
        # í”„ë ˆì„ ë¶„ì„ ìš”ì•½
        frame_summary = "\n\n".join([
            f"[í”„ë ˆì„ {i+1}] {analysis}"
            for i, analysis in enumerate(frame_analyses)
        ])

        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì˜ìƒ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë§ˆì¸ë“œë§µ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ë§ˆì¸ë“œë§µì€ ê¹Šì´ ì œí•œ ì—†ì´ ììœ ë¡­ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë‚˜ ì„¤ëª…ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**"""

        # ì‚¬ìš©ì ì¿¼ë¦¬ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if user_query:
            analysis_instruction = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸: "{user_query}"

ìœ„ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í˜•íƒœë¡œ ë§ˆì¸ë“œë§µì„ êµ¬ì„±í•˜ì„¸ìš”.
ì˜ìƒì—ì„œ ë‚˜ì˜¨ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µ, ì•„ì´ë””ì–´, ê´€ë ¨ ê°œë…ë“¤ì„ ê³„ì¸µì ìœ¼ë¡œ ì¡°ì§í™”í•˜ì„¸ìš”.
"""
        else:
            analysis_instruction = """
ì˜ìƒì˜ ì£¼ìš” ë‚´ìš©ì„ ê³„ì¸µì ìœ¼ë¡œ ì¡°ì§í™”í•˜ì—¬ ë§ˆì¸ë“œë§µì„ êµ¬ì„±í•˜ì„¸ìš”.
ì£¼ìš” ì£¼ì œ, í•˜ìœ„ ê°œë…, ì„¸ë¶€ ë‚´ìš© ë“±ì„ ììœ ë¡­ê²Œ êµ¬ì¡°í™”í•˜ì„¸ìš”.
"""

        user_prompt = f"""ë‹¤ìŒì€ YouTube ì˜ìƒ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

**ì˜ìƒ ì œëª©:** {video_title}

**ì‹œê° ì •ë³´ (ì£¼ìš” í”„ë ˆì„):**
{frame_summary}

**ìŒì„±/ìë§‰ ë‚´ìš©:**
{transcript}

---

{analysis_instruction}

**í•„ìˆ˜ ê·œì¹™:**
1. ë§ˆì¸ë“œë§µì˜ ë£¨íŠ¸ ë…¸ë“œëŠ” ë°˜ë“œì‹œ ì˜ìƒ ì œëª©ì´ì–´ì•¼ í•©ë‹ˆë‹¤
2. ê° ë…¸ë“œëŠ” ë°˜ë“œì‹œ "keyword"ì™€ "description" í•„ë“œë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤
   - **keyword**: ì§§ê³  ëª…í™•í•œ í•µì‹¬ ë‹¨ì–´ (3-5 ë‹¨ì–´)
   - **description**: ì˜ìƒì—ì„œ ë‚˜ì˜¨ êµ¬ì²´ì ì¸ ì‚¬ì‹¤, ìˆ˜ì¹˜, ì¸ìš©ë¬¸ ë“± ì‹¤ì œ ë‚´ìš© (ì¶”ìƒì ì¸ ì„¤ëª… ê¸ˆì§€!)
3. "children" ë°°ì—´ë¡œ í•˜ìœ„ ë…¸ë“œë“¤ì„ í‘œí˜„í•©ë‹ˆë‹¤ (ìì‹ì´ ì—†ìœ¼ë©´ null)
4. ê¹Šì´ ì œí•œ ì—†ì´ í•„ìš”í•œ ë§Œí¼ ê³„ì¸µì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤
5. **ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë°±í‹±(```), ì„¤ëª…, ì£¼ì„ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”**

**ì¤‘ìš”: keywordë¥¼ JSONì˜ keyë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”! ë°˜ë“œì‹œ "keyword" í•„ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!**

**Description ì‘ì„± ê·œì¹™:**
- âŒ ë‚˜ìœ ì˜ˆ: "íŠ¸ëŸ¼í”„ì˜ ê´€ì„¸ ì¸ìƒì— ëŒ€í•´ ì„¤ëª…", "ë¬´ì—­ ê´€ê³„ì— ëŒ€í•´ ì„¤ëª…"
- âœ… ì¢‹ì€ ì˜ˆ: "íŠ¸ëŸ¼í”„ê°€ ìºë‚˜ë‹¤ì— ì¶”ê°€ 10% ê´€ì„¸ ë¶€ê³¼, ë ˆì´ê±´ ê´‘ê³ ë¥¼ ì˜¤ë„ì ì´ë¼ê³  ë¹„íŒ", "ë¯¸-ìº ë¬´ì—­ ê·œëª¨ $762B, ìºë‚˜ë‹¤ëŠ” ë¯¸êµ­ì˜ 2ìœ„ ë¬´ì—­ íŒŒíŠ¸ë„ˆ"
- ë°˜ë“œì‹œ ì˜ìƒì—ì„œ ì–¸ê¸‰ëœ **êµ¬ì²´ì ì¸ ì‚¬ì‹¤, ìˆ«ì, ë‚ ì§œ, ì¸ë¬¼, ì¸ìš©ë¬¸**ì„ í¬í•¨í•˜ì„¸ìš”!

ì˜¬ë°”ë¥¸ í˜•ì‹:
{{
  "keyword": "ì˜ìƒ ì œëª©",
  "description": "ì˜ìƒì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…",
  "children": [
    {{
      "keyword": "ì£¼ìš” ì£¼ì œ 1",
      "description": "ì£¼ì œì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…",
      "children": [
        {{
          "keyword": "í•˜ìœ„ ê°œë… 1-1",
          "description": "í•˜ìœ„ ê°œë…ì— ëŒ€í•œ ì„¤ëª…",
          "children": null
        }}
      ]
    }},
    {{
      "keyword": "ì£¼ìš” ì£¼ì œ 2",
      "description": "ì£¼ì œì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…",
      "children": null
    }}
  ]
}}

í‹€ë¦° í˜•ì‹ (ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”):
{{
  "ì˜ìƒ ì œëª©": {{
    "description": "...",
    "children": [...]
  }}
}}

ì´ì œ ìœ„ì˜ ì˜¬ë°”ë¥¸ í˜•ì‹ì— ë§ì¶° ìœ íš¨í•œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""

        result = self.generate(
            user_prompt,
            system_prompt=system_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )

        return result.strip()

    def get_vram_usage(self) -> Dict[str, float]:
        """í˜„ì¬ VRAM ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        if torch.cuda.is_available():
            return {
                "allocated_gb": torch.cuda.memory_allocated() / 1024**3,
                "reserved_gb": torch.cuda.memory_reserved() / 1024**3
            }
        return {"allocated_gb": 0.0, "reserved_gb": 0.0}

    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")


def main():
    """í…ŒìŠ¤íŠ¸ ì˜ˆì œ"""
    logger.info("=" * 60)
    logger.info("ğŸ¦™ Llama 3.1 8B Text Analyzer í…ŒìŠ¤íŠ¸")
    logger.info("=" * 60)

    # Analyzer ì´ˆê¸°í™”
    analyzer = LlamaTextAnalyzer(quantization="int4")

    # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
    logger.info("\nğŸ“ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    result = analyzer.generate(
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ 3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        temperature=0.7
    )
    logger.info(f"\nğŸ¤– ìƒì„± ê²°ê³¼:\n{result}\n")

    # VRAM ì‚¬ìš©ëŸ‰ í™•ì¸
    vram = analyzer.get_vram_usage()
    logger.info(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰: {vram['allocated_gb']:.2f} GB")

    # í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    logger.info("\nğŸ“Œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸...")
    sample_text = """
    ì¸ê³µì§€ëŠ¥ì€ í˜„ëŒ€ ì‚¬íšŒì—ì„œ ì ì  ë” ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    ì˜ë£Œ, êµìœ¡, ê¸ˆìœµ, ì œì¡°ì—… ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ AIê°€ í™œìš©ë˜ê³  ìˆìœ¼ë©°,
    íŠ¹íˆ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ìœ¼ë¡œ ìì—°ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
    í•˜ì§€ë§Œ AI ìœ¤ë¦¬ì™€ ê°œì¸ì •ë³´ ë³´í˜¸ ë¬¸ì œë„ í•¨ê»˜ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    points = analyzer.extract_key_points(sample_text, max_points=3)
    logger.info("í•µì‹¬ í¬ì¸íŠ¸:")
    for i, point in enumerate(points, 1):
        logger.info(f"{i}. {point}")

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    analyzer.cleanup()


if __name__ == "__main__":
    main()
