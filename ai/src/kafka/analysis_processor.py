"""
Kafka ë©”ì‹œì§€ ë¶„ì„ ì²˜ë¦¬ ëª¨ë“ˆ
- INITIAL: VIDEO/IMAGE/TEXT ë¶„ì„í•˜ì—¬ ë§ˆì¸ë“œë§µ ë…¸ë“œ ìƒì„±
- CONTEXTUAL: 3ê°œ ìì‹ ë…¸ë“œ ìƒì„±
"""
import logging
from typing import Dict, Any, List, Optional
from src.core.transcript_extractor import TranscriptExtractor
from src.core.llama_text_analyzer import LlamaTextAnalyzer
from src.core.llama_vision_analyzer import LlamaVisionAnalyzer
from src.core.image_analyzer import ImageAnalyzer

logger = logging.getLogger(__name__)


class AnalysisProcessor:
    """Kafka ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ê¸°"""

    def __init__(self, text_analyzer: LlamaTextAnalyzer, vision_analyzer: LlamaVisionAnalyzer):
        """
        Args:
            text_analyzer: LLM í…ìŠ¤íŠ¸ ë¶„ì„ê¸°
            vision_analyzer: LLM ë¹„ì „ ë¶„ì„ê¸°
        """
        self.text_analyzer = text_analyzer
        self.vision_analyzer = vision_analyzer
        self.image_analyzer = ImageAnalyzer()

    def process_initial(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        INITIAL ë¶„ì„ ì²˜ë¦¬ - contentTypeì— ë”°ë¼ VIDEO/IMAGE/TEXT ì²˜ë¦¬

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€
                - workspaceId: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ID
                - nodeId: ë…¸ë“œ ID
                - contentType: 'VIDEO', 'IMAGE', 'TEXT'
                - contentUrl: ì»¨í…ì¸  URL (TEXTì¼ ê²½ìš° null)
                - prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # workspaceIdë¥¼ intë¡œ ë³€í™˜ (Java longê³¼ í˜¸í™˜)
        workspace_id = int(request['workspaceId'])
        node_id = request['nodeId']
        content_url = request.get('contentUrl')
        content_type = request.get('contentType', 'TEXT')
        prompt = request.get('prompt', '')

        logger.info(f"INITIAL ë¶„ì„ ì‹œì‘: workspaceId={workspace_id}, nodeId={node_id}, "
                   f"contentType={content_type}")

        try:
            # ContentTypeì— ë”°ë¼ ì»¨í…ì¸  ì¶”ì¶œ
            content_text = ""

            if content_type == 'VIDEO':
                # VIDEO: YouTube ìë§‰ ì¶”ì¶œ
                if not content_url:
                    raise ValueError("VIDEO íƒ€ì…ì¸ë° contentUrlì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ“¹ VIDEO ë¶„ì„: {content_url}")
                transcript_result = TranscriptExtractor.get_transcript(content_url)
                if transcript_result['success']:
                    content_text = transcript_result['full_text']
                    logger.info(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(content_text)}ì")
                else:
                    error_msg = transcript_result.get('error', 'Unknown error')
                    logger.warning(f"âš ï¸  ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")
                    raise ValueError(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")

            elif content_type == 'IMAGE':
                # IMAGE: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ Vision ëª¨ë¸ë¡œ ë¶„ì„
                if not content_url:
                    raise ValueError("IMAGE íƒ€ì…ì¸ë° contentUrlì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ–¼ï¸  IMAGE ë¶„ì„: {content_url}")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                image_path = self.image_analyzer.download_image(content_url)
                if not image_path:
                    raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {content_url}")

                logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {image_path}")

                # Vision ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
                vision_prompt = f"""ì´ ì´ë¯¸ì§€ë¥¼ ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ê³  í¬ê´„ì ì¸ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: {prompt}

ë‹¤ìŒ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ë‚´ìš©ê³¼ í…ìŠ¤íŠ¸ (ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì£¼ì„¸ìš”)
2. ì‹œê°ì  ìš”ì†Œë“¤ (ì°¨íŠ¸, ê·¸ë˜í”„, ë‹¤ì´ì–´ê·¸ë¨, ë¡œê³ , ì‹¬ë³¼)
3. ì œì‹œëœ í•µì‹¬ ê°œë…ê³¼ ì•„ì´ë””ì–´
4. ê³„ì¸µ êµ¬ì¡°ë‚˜ ì¡°ì§ ë°©ì‹

ëª…í™•í•˜ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”."""

                content_text = self.vision_analyzer.analyze_image(
                    image=image_path,
                    prompt=vision_prompt
                )
                logger.info(f"âœ… Vision ë¶„ì„ ì™„ë£Œ: {len(content_text)}ì")

            elif content_type == 'TEXT':
                # TEXT: promptë§Œ ì‚¬ìš©
                logger.info(f"ğŸ“ TEXT ë¶„ì„: promptë§Œ ì‚¬ìš©")
                content_text = ""  # ë¹ˆ ë¬¸ìì—´, promptë§Œ ì‚¬ìš©

            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” contentType: {content_type}")

            # ë§ˆì¸ë“œë§µ ìƒì„± í”„ë¡¬í”„íŠ¸
            system_prompt = """You are an expert at analyzing content and generating hierarchical mindmap nodes in JSON format.

CRITICAL: You MUST output ONLY valid JSON. No explanations, no markdown, no code blocks.

Generate nodes with appropriate depth based on content complexity:
- Main category nodes under root nodeId={node_id}
- Sub-nodes under each category as needed
- Logical hierarchical structure

Output this EXACT JSON structure:
{{
  "aiSummary": "Brief 1-2 sentence summary",
  "nodes": [
    {{"tempId": "temp-1", "parentId": {node_id}, "keyword": "Category 1", "memo": "Description"}},
    {{"tempId": "temp-2", "parentId": {node_id}, "keyword": "Category 2", "memo": "Description"}},
    {{"tempId": "temp-3", "parentId": "temp-1", "keyword": "Subtopic 1-1", "memo": "Details"}},
    {{"tempId": "temp-4", "parentId": "temp-1", "keyword": "Subtopic 1-2", "memo": "Details"}}
  ]
}}

Rules:
1. tempId: "temp-1", "temp-2", etc (sequential)
2. keyword: 2-5 words, concise
3. memo: 10-30 characters, specific
4. Output ONLY JSON, nothing else
""".format(node_id=node_id)

            # contentTypeì— ë”°ë¼ user prompt êµ¬ì„±
            if content_text:
                user_prompt_text = f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ì½˜í…ì¸  ë‚´ìš©:
{content_text[:5000]}

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ë§ˆì¸ë“œë§µ ë…¸ë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì½˜í…ì¸ ì˜ ë³µì¡ë„ì™€ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ê°œìˆ˜ì™€ ê¹Šì´ë¡œ êµ¬ì„±í•˜ì„¸ìš”."""
            else:
                # TEXT íƒ€ì…: promptë§Œ ì‚¬ìš©
                user_prompt_text = f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ìœ„ ì£¼ì œ/ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ë§ˆì¸ë“œë§µ ë…¸ë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì£¼ì œì˜ ë³µì¡ë„ì™€ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ê°œìˆ˜ì™€ ê¹Šì´ë¡œ êµ¬ì„±í•˜ì„¸ìš”."""

            # LLM í˜¸ì¶œ (JSON ìƒì„±ì„ ìœ„í•´ ë‚®ì€ temperature)
            logger.info("ğŸ¤– LLMìœ¼ë¡œ ë§ˆì¸ë“œë§µ ìƒì„± ì¤‘...")
            response = self.text_analyzer.generate(
                prompt=user_prompt_text,
                system_prompt=system_prompt,
                max_tokens=2048,
                temperature=0.2,
                top_p=0.85,
                top_k=40,
                repetition_penalty=1.15
            )

            # ì‘ë‹µ ê²€ì¦ ë° ë¡œê¹…
            logger.info(f"ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
            logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:\n{response[:500]}")

            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not response or not response.strip():
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

            # JSON íŒŒì‹± ì „ì²˜ë¦¬
            response = response.strip()

            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```json' in response:
                response = response.split('```json')[1]
            if '```' in response:
                response = response.split('```')[0]

            # JSON ê°ì²´ ì¶”ì¶œ (ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€)
            start_idx = response.find('{')
            end_idx = response.rfind('}')

            if start_idx == -1 or end_idx == -1:
                raise ValueError("ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            response = response[start_idx:end_idx+1].strip()

            # JSON íŒŒì‹± ì‹œë„
            import json
            try:
                result_data = json.loads(response)
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨!")
                logger.error(f"íŒŒì‹±í•˜ë ¤ë˜ ë¬¸ìì—´:\n{response}")
                raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

            # ê²°ê³¼ ê²€ì¦
            if 'aiSummary' not in result_data or 'nodes' not in result_data:
                raise ValueError("ì‘ë‹µì— aiSummary ë˜ëŠ” nodesê°€ ì—†ìŠµë‹ˆë‹¤")

            logger.info(f"âœ… ìƒì„±ëœ ë…¸ë“œ ê°œìˆ˜: {len(result_data['nodes'])}ê°œ")

            # Kafka ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            kafka_response = {
                "workspaceId": workspace_id,
                "aiSummary": result_data['aiSummary'],
                "status": "SUCCESS",
                "nodes": result_data['nodes']
            }

            logger.info(f"âœ… INITIAL ë¶„ì„ ì™„ë£Œ ({content_type}): {len(result_data['nodes'])}ê°œ ë…¸ë“œ ìƒì„±")
            return kafka_response

        except Exception as e:
            logger.error(f"âŒ INITIAL ë¶„ì„ ì‹¤íŒ¨ ({content_type}): {e}", exc_info=True)
            return {
                "workspaceId": workspace_id,
                "status": "FAILED",
                "error": str(e)
            }

    def process_contextual(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        CONTEXTUAL ë¶„ì„ ì²˜ë¦¬ - 3ê°œ ìì‹ ë…¸ë“œ ìƒì„±
        contentTypeì— ë”°ë¼ VIDEO/IMAGE/TEXT ì²˜ë¦¬

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€
                - workspaceId: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ID
                - nodeId: í™•ì¥í•  ë…¸ë“œ ID
                - nodes: ë¶€ëª¨ ë…¸ë“œë“¤ (ë‚´ ë…¸ë“œ í¬í•¨)
                - contentType: 'VIDEO', 'IMAGE', 'TEXT'

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # workspaceIdë¥¼ intë¡œ ë³€í™˜ (Java longê³¼ í˜¸í™˜)
        workspace_id = int(request['workspaceId'])
        node_id = request['nodeId']
        parent_nodes = request.get('nodes', [])
        content_type = request.get('contentType', 'TEXT')

        logger.info(f"CONTEXTUAL ë¶„ì„ ì‹œì‘: workspaceId={workspace_id}, nodeId={node_id}, "
                   f"contentType={content_type}, ë…¸ë“œ ì²´ì¸ ìˆ˜={len(parent_nodes)}")

        try:
            # 1. nodeIdë¡œ í™•ì¥í•  ë…¸ë“œ ì°¾ê¸°
            target_node = None
            for node in parent_nodes:
                if node.get('nodeId') == node_id or node.get('id') == node_id:
                    target_node = node
                    break

            if not target_node:
                raise ValueError(f"nodeId={node_id}ì¸ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            logger.info(f"í™•ì¥í•  ë…¸ë“œ ì°¾ìŒ: keyword={target_node.get('keyword')}")

            # 2. ContentTypeì— ë”°ë¼ ì»¨í…ì¸  ì¶”ì¶œ
            content_text = ""

            if content_type == 'VIDEO':
                # VIDEO: keywordì—ì„œ YouTube URL ì¶”ì¶œ
                content_url = target_node.get('keyword', '')
                if not content_url:
                    raise ValueError("VIDEO íƒ€ì…ì¸ë° keywordì— URLì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ“¹ VIDEO ë¶„ì„: {content_url}")
                transcript_result = TranscriptExtractor.get_transcript(content_url)
                if transcript_result['success']:
                    content_text = transcript_result['full_text']
                    logger.info(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(content_text)}ì")
                else:
                    error_msg = transcript_result.get('error', 'Unknown error')
                    logger.warning(f"âš ï¸  ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")
                    raise ValueError(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")

            elif content_type == 'IMAGE':
                # IMAGE: keywordì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
                content_url = target_node.get('keyword', '')
                if not content_url:
                    raise ValueError("IMAGE íƒ€ì…ì¸ë° keywordì— URLì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ–¼ï¸  IMAGE ë¶„ì„: {content_url}")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                image_path = self.image_analyzer.download_image(content_url)
                if not image_path:
                    raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {content_url}")

                logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {image_path}")

                # Vision ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
                vision_prompt = f"""ì´ ì´ë¯¸ì§€ë¥¼ ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ê³  í¬ê´„ì ì¸ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:
1. ì£¼ìš” ë‚´ìš©ê³¼ í…ìŠ¤íŠ¸ (ë³´ì´ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì£¼ì„¸ìš”)
2. ì‹œê°ì  ìš”ì†Œë“¤ (ì°¨íŠ¸, ê·¸ë˜í”„, ë‹¤ì´ì–´ê·¸ë¨, ë¡œê³ , ì‹¬ë³¼)
3. ì œì‹œëœ í•µì‹¬ ê°œë…ê³¼ ì•„ì´ë””ì–´
4. ìƒì„¸í•œ í•˜ìœ„ ì£¼ì œë‚˜ êµ¬ì„± ìš”ì†Œ

ëª…í™•í•˜ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”. ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”."""

                content_text = self.vision_analyzer.analyze_image(
                    image=image_path,
                    prompt=vision_prompt
                )
                logger.info(f"âœ… Vision ë¶„ì„ ì™„ë£Œ: {len(content_text)}ì")

            elif content_type == 'TEXT':
                # TEXT: ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ë§Œ ì‚¬ìš©
                logger.info(f"ğŸ“ TEXT ë¶„ì„: ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ ì‚¬ìš©")
                content_text = ""

            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” contentType: {content_type}")

            # 3. ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ êµ¬ì„±
            context_text = self._build_context_from_parents(parent_nodes)

            # 4. ë§ˆì¸ë“œë§µ ìƒì„± í”„ë¡¬í”„íŠ¸
            system_prompt = """You are an expert at generating exactly 3 child nodes based on parent context.

CRITICAL: You MUST output ONLY valid JSON. No explanations, no markdown, no code blocks.

Output this EXACT JSON structure:
{{
  "nodes": [
    {{"keyword": "Subtopic 1", "memo": "Specific description"}},
    {{"keyword": "Subtopic 2", "memo": "Specific description"}},
    {{"keyword": "Subtopic 3", "memo": "Specific description"}}
  ]
}}

Rules:
1. Generate EXACTLY 3 nodes
2. keyword: 2-5 words, concise
3. memo: 10-30 characters, specific
4. Follow the flow of parent nodes logically
5. Output ONLY JSON, nothing else
"""

            # contentTypeì— ë”°ë¼ user prompt êµ¬ì„±
            if content_text:
                # VIDEO/IMAGE: ì»¨í…ì¸  ë‚´ìš© + ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥
                user_prompt_text = f"""ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥:
{context_text}

ì»¨í…ì¸  ë¶„ì„ ê²°ê³¼:
{content_text[:3000]}

ìœ„ ë¬¸ë§¥ê³¼ ì»¨í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œì˜ 3ê°œ í•˜ìœ„ ë…¸ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."""
            else:
                # TEXT: ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ë§Œ ì‚¬ìš©
                user_prompt_text = f"""ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥:
{context_text}

ìœ„ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œì˜ 3ê°œ í•˜ìœ„ ë…¸ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ (JSON ìƒì„±ì„ ìœ„í•´ ë‚®ì€ temperature)
            logger.info("ğŸ¤– LLMìœ¼ë¡œ 3ê°œ ìì‹ ë…¸ë“œ ìƒì„± ì¤‘...")
            response = self.text_analyzer.generate(
                prompt=user_prompt_text,
                system_prompt=system_prompt,
                max_tokens=1024,
                temperature=0.2,
                top_p=0.85,
                top_k=40,
                repetition_penalty=1.15
            )

            # ì‘ë‹µ ê²€ì¦ ë° ë¡œê¹…
            logger.info(f"ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
            logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:\n{response[:500]}")

            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not response or not response.strip():
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

            # JSON íŒŒì‹± ì „ì²˜ë¦¬
            response = response.strip()

            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```json' in response:
                response = response.split('```json')[1]
            if '```' in response:
                response = response.split('```')[0]

            # JSON ê°ì²´ ì¶”ì¶œ (ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€)
            start_idx = response.find('{')
            end_idx = response.rfind('}')

            if start_idx == -1 or end_idx == -1:
                raise ValueError("ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            response = response[start_idx:end_idx+1].strip()

            # JSON íŒŒì‹± ì‹œë„
            import json
            try:
                result_data = json.loads(response)
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨!")
                logger.error(f"íŒŒì‹±í•˜ë ¤ë˜ ë¬¸ìì—´:\n{response}")
                raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

            # ê²°ê³¼ ê²€ì¦
            if 'nodes' not in result_data:
                raise ValueError("ì‘ë‹µì— nodesê°€ ì—†ìŠµë‹ˆë‹¤")

            if len(result_data['nodes']) != 3:
                logger.warning(f"ë…¸ë“œ ê°œìˆ˜ê°€ 3ê°œê°€ ì•„ë‹™ë‹ˆë‹¤: {len(result_data['nodes'])}ê°œ")
                # 3ê°œë¡œ ìë¥´ê¸° ë˜ëŠ” ì±„ìš°ê¸°
                if len(result_data['nodes']) > 3:
                    result_data['nodes'] = result_data['nodes'][:3]
                elif len(result_data['nodes']) < 3:
                    # ë¶€ì¡±í•œ ë§Œí¼ ê¸°ë³¸ ë…¸ë“œ ì¶”ê°€
                    for i in range(3 - len(result_data['nodes'])):
                        result_data['nodes'].append({
                            "keyword": f"ì¶”ê°€ ì£¼ì œ {i+1}",
                            "memo": "ìƒì„¸ ë‚´ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤"
                        })

            # Kafka ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            kafka_response = {
                "workspaceId": workspace_id,
                "nodeId": node_id,
                "status": "SUCCESS",
                "nodes": result_data['nodes']
            }

            logger.info(f"âœ… CONTEXTUAL ë¶„ì„ ì™„ë£Œ: {len(result_data['nodes'])}ê°œ ë…¸ë“œ ìƒì„±")
            return kafka_response

        except Exception as e:
            logger.error(f"âŒ CONTEXTUAL ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "workspaceId": workspace_id,
                "status": "FAILED",
                "error": str(e)
            }

    def _build_context_from_parents(self, parent_nodes: List[Dict]) -> str:
        """
        ë¶€ëª¨ ë…¸ë“œë“¤ë¡œë¶€í„° ë¬¸ë§¥ í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            parent_nodes: ë¶€ëª¨ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¬¸ë§¥ í…ìŠ¤íŠ¸
        """
        if not parent_nodes:
            return "(ë¶€ëª¨ ë…¸ë“œ ì—†ìŒ)"

        context_lines = []
        for i, node in enumerate(parent_nodes, 1):
            keyword = node.get('keyword', '')
            memo = node.get('memo', '')
            context_lines.append(f"{i}. {keyword}: {memo}")

        return "\n".join(context_lines)

    def process_request(self, request: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Kafka ìš”ì²­ ì²˜ë¦¬ (ë¶„ì„ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°)

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€

        Returns:
            ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None
        """
        analysis_type = request.get('analysisType')

        if analysis_type == 'INITIAL':
            return self.process_initial(request)
        elif analysis_type == 'CONTEXTUAL':
            return self.process_contextual(request)
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ì„ íƒ€ì…: {analysis_type}")
            return {
                "workspaceId": request.get('workspaceId'),
                "status": "FAILED",
                "error": f"Unknown analysis type: {analysis_type}"
            }
