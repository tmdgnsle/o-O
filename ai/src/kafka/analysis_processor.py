"""
Kafka ë©”ì‹œì§€ ë¶„ì„ ì²˜ë¦¬ ëª¨ë“ˆ
- INITIAL: VIDEO/IMAGE/TEXT ë¶„ì„í•˜ì—¬ ë§ˆì¸ë“œë§µ ë…¸ë“œ ìƒì„±
- CONTEXTUAL: 3ê°œ ìì‹ ë…¸ë“œ ìƒì„±
"""
import logging
from typing import Dict, Any, List, Optional
from src.core.transcript_extractor import TranscriptExtractor
from src.core.llama_text_analyzer import LlamaTextAnalyzer
from src.core.llama_vision_analyzer import LlamaVisionAnalyzer
from src.core.image_analyzer import ImageAnalyzer

logger = logging.getLogger(__name__)


class AnalysisProcessor:
    """Kafka ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ê¸°"""

    def __init__(self, text_analyzer: LlamaTextAnalyzer, vision_analyzer: LlamaVisionAnalyzer):
        """
        Args:
            text_analyzer: LLM í…ìŠ¤íŠ¸ ë¶„ì„ê¸°
            vision_analyzer: LLM ë¹„ì „ ë¶„ì„ê¸°
        """
        self.text_analyzer = text_analyzer
        self.vision_analyzer = vision_analyzer
        self.image_analyzer = ImageAnalyzer()

    def process_initial(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        INITIAL ë¶„ì„ ì²˜ë¦¬ - contentTypeì— ë”°ë¼ VIDEO/IMAGE/TEXT ì²˜ë¦¬

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€
                - workspaceId: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ID
                - nodeId: ë…¸ë“œ ID
                - contentType: 'VIDEO', 'IMAGE', 'TEXT'
                - contentUrl: ì»¨í…ì¸  URL (TEXTì¼ ê²½ìš° null)
                - prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # workspaceIdë¥¼ intë¡œ ë³€í™˜ (Java longê³¼ í˜¸í™˜)
        workspace_id = int(request['workspaceId'])
        node_id = request['nodeId']
        content_url = request.get('contentUrl')
        content_type = request.get('contentType', 'TEXT')
        prompt = request.get('prompt', '')

        logger.info(f"INITIAL ë¶„ì„ ì‹œì‘: workspaceId={workspace_id}, nodeId={node_id}, "
                   f"contentType={content_type}")

        try:
            # ContentTypeì— ë”°ë¼ ì»¨í…ì¸  ì¶”ì¶œ
            content_text = ""

            if content_type == 'VIDEO':
                # VIDEO: YouTube ìë§‰ ì¶”ì¶œ
                if not content_url:
                    raise ValueError("VIDEO íƒ€ì…ì¸ë° contentUrlì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ“¹ VIDEO ë¶„ì„: {content_url}")
                transcript_result = TranscriptExtractor.get_transcript(content_url)
                if transcript_result['success']:
                    content_text = transcript_result['full_text']
                    logger.info(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(content_text)}ì")
                else:
                    error_msg = transcript_result.get('error', 'Unknown error')
                    logger.warning(f"âš ï¸  ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")
                    raise ValueError(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")

            elif content_type == 'IMAGE':
                # IMAGE: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ Vision ëª¨ë¸ë¡œ ë¶„ì„
                if not content_url:
                    raise ValueError("IMAGE íƒ€ì…ì¸ë° contentUrlì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ–¼ï¸  IMAGE ë¶„ì„: {content_url}")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                image_path = self.image_analyzer.download_image(content_url)
                if not image_path:
                    raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {content_url}")

                logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {image_path}")

                # Vision ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
                vision_prompt = f"""Analyze this image concisely in 3-5 sentences. Focus on unique information only.

User request: {prompt}

Describe:
1. Main visible text and content
2. Key visual elements (charts, diagrams, logos)
3. Core concepts presented

Be concise. Never repeat yourself. Stop after describing main points."""

                content_text = self.vision_analyzer.analyze_image(
                    image=image_path,
                    prompt=vision_prompt,
                    max_tokens=512,
                    temperature=0.3
                )
                logger.info(f"âœ… Vision ë¶„ì„ ì™„ë£Œ: {len(content_text)}ì")
                logger.info(f"ğŸ“„ Vision ë¶„ì„ ë‚´ìš©:\n{content_text}")

                # ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
                self.image_analyzer.cleanup(image_path)

            elif content_type == 'TEXT':
                # TEXT: promptë§Œ ì‚¬ìš©
                logger.info(f"ğŸ“ TEXT ë¶„ì„: promptë§Œ ì‚¬ìš©")
                content_text = ""  # ë¹ˆ ë¬¸ìì—´, promptë§Œ ì‚¬ìš©

            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” contentType: {content_type}")

            # ë§ˆì¸ë“œë§µ ìƒì„± í”„ë¡¬í”„íŠ¸
            system_prompt = """You are an expert at analyzing content and generating hierarchical mindmap nodes in JSON format.

CRITICAL REQUIREMENTS:
1. Output ONLY valid JSON - No explanations, no markdown, no code blocks
2. MUST include "title", "aiSummary", and "nodes" fields
3. The root node already exists - DO NOT create a top-level category node
4. Each field must appear ONLY ONCE per node (no duplicate keys)

Generate nodes with appropriate depth (5-15 nodes recommended):
- All first-level nodes MUST have parentId={node_id}
- Create sub-nodes under categories as needed
- Build a logical hierarchical structure

REQUIRED JSON structure (copy this format exactly):
{{
  "title": "Concise mindmap title (3-10 words)",
  "aiSummary": "Brief 1-2 sentence summary of the content",
  "nodes": [
    {{"tempId": "temp-1", "parentId": {node_id}, "keyword": "Main Topic 1", "memo": "Detailed description"}},
    {{"tempId": "temp-2", "parentId": {node_id}, "keyword": "Main Topic 2", "memo": "Detailed description"}},
    {{"tempId": "temp-3", "parentId": "temp-1", "keyword": "Subtopic 1-1", "memo": "Specific details"}},
    {{"tempId": "temp-4", "parentId": "temp-1", "keyword": "Subtopic 1-2", "memo": "Specific details"}}
  ]
}}

MANDATORY Rules:
1. "title" field is REQUIRED - a concise title for the entire mindmap (3-10 words)
2. "aiSummary" field is REQUIRED at the top level
3. tempId: "temp-1", "temp-2", etc (sequential)
4. parentId: MUST be {node_id} or another tempId (NEVER null, NEVER duplicate)
5. keyword: 2-5 words, concise (NEVER empty)
6. memo: 10-50 characters, informative (NEVER empty)
7. Each node must have exactly 4 fields: tempId, parentId, keyword, memo
8. DO NOT repeat field names within a single node
""".format(node_id=node_id)

            # contentTypeì— ë”°ë¼ user prompt êµ¬ì„±
            if content_text:
                user_prompt_text = f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ì½˜í…ì¸  ë‚´ìš©:
{content_text[:5000]}

ìœ„ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ë§ˆì¸ë“œë§µ ë…¸ë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì½˜í…ì¸ ì˜ ë³µì¡ë„ì™€ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ê°œìˆ˜ì™€ ê¹Šì´ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

IMPORTANT: keywordì™€ memoëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. titleê³¼ aiSummaryë„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
            else:
                # TEXT íƒ€ì…: promptë§Œ ì‚¬ìš©
                user_prompt_text = f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ìœ„ ì£¼ì œ/ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ë§ˆì¸ë“œë§µ ë…¸ë“œë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì£¼ì œì˜ ë³µì¡ë„ì™€ ë‚´ìš©ì— ë”°ë¼ ì ì ˆí•œ ê°œìˆ˜ì™€ ê¹Šì´ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

IMPORTANT: keywordì™€ memoëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. titleê³¼ aiSummaryë„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ (JSON ìƒì„±ì„ ìœ„í•´ ë‚®ì€ temperature)
            logger.info("ğŸ¤– LLMìœ¼ë¡œ ë§ˆì¸ë“œë§µ ìƒì„± ì¤‘...")
            response = self.text_analyzer.generate(
                prompt=user_prompt_text,
                system_prompt=system_prompt,
                max_tokens=2048,
                temperature=0.2,
                top_p=0.85,
                top_k=40,
                repetition_penalty=1.15
            )

            # ì‘ë‹µ ê²€ì¦ ë° ë¡œê¹…
            logger.info(f"ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
            logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:\n{response[:500]}")

            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not response or not response.strip():
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

            # JSON íŒŒì‹± ì „ì²˜ë¦¬
            response = response.strip()

            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```json' in response:
                response = response.split('```json')[1]
            if '```' in response:
                response = response.split('```')[0]

            # JSON ê°ì²´ ì¶”ì¶œ (ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€)
            start_idx = response.find('{')
            end_idx = response.rfind('}')

            if start_idx == -1 or end_idx == -1:
                raise ValueError("ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            response = response[start_idx:end_idx+1].strip()

            # JSON íŒŒì‹± ì‹œë„
            import json
            import re
            try:
                result_data = json.loads(response)
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì˜¤ë¥˜ ë³µêµ¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤: {e}")

                # ì¤‘ë³µ í•„ë“œ ì œê±°: ê°™ì€ ê°ì²´ ë‚´ì—ì„œ ë™ì¼ í•„ë“œê°€ 2ë²ˆ ë‚˜ì˜¤ë©´ ì²« ë²ˆì§¸ë§Œ ìœ ì§€
                # ê° ë…¸ë“œ ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì¤‘ë³µ ì œê±°
                try:
                    # ìˆ˜ë™ìœ¼ë¡œ ë…¸ë“œë³„ë¡œ íŒŒì‹±
                    fixed_response = response
                    # ì¤‘ë³µ parentId íŒ¨í„´ ì°¾ê¸°
                    fixed_response = re.sub(
                        r'("parentId"\s*:\s*"[^"]*"),\s*("keyword"[^}]*)"parentId"\s*:\s*"[^"]*"',
                        r'\1, \2',
                        fixed_response
                    )
                    result_data = json.loads(fixed_response)
                    logger.info("âœ… JSON ì˜¤ë¥˜ ë³µêµ¬ ì„±ê³µ")
                except Exception as e2:
                    logger.error(f"JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨!")
                    logger.error(f"ì›ë³¸:\n{response[:500]}")
                    raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e2}")

            # ê²°ê³¼ ê²€ì¦ ë° ë³´ì •
            if 'nodes' not in result_data:
                raise ValueError("ì‘ë‹µì— nodesê°€ ì—†ìŠµë‹ˆë‹¤")

            # titleì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            if 'title' not in result_data or not result_data['title']:
                logger.warning("âš ï¸ titleì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                result_data['title'] = f"{content_type} ë¶„ì„ ë§ˆì¸ë“œë§µ"

            # aiSummaryê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            if 'aiSummary' not in result_data or not result_data['aiSummary']:
                logger.warning("âš ï¸ aiSummaryê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                result_data['aiSummary'] = f"{content_type} ì»¨í…ì¸  ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."

            logger.info(f"âœ… ìƒì„±ëœ ë…¸ë“œ ê°œìˆ˜: {len(result_data['nodes'])}ê°œ")

            # í›„ì²˜ë¦¬: parentIdê°€ None/nullì¸ ë…¸ë“œë¥¼ nodeIdë¡œ ë³€ê²½
            for node in result_data['nodes']:
                if node.get('parentId') is None or node.get('parentId') == 'null':
                    logger.warning(f"âš ï¸ ë…¸ë“œ {node.get('tempId')}ì˜ parentIdê°€ nullì…ë‹ˆë‹¤. nodeId({node_id})ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                    node['parentId'] = node_id

                # memoê°€ ë¹„ì–´ìˆìœ¼ë©´ keyword ê¸°ë°˜ ê¸°ë³¸ ì„¤ëª… ì¶”ê°€
                if not node.get('memo') or node.get('memo').strip() == '':
                    default_memo = f"{node.get('keyword', 'Topic')}ì— ëŒ€í•œ ë‚´ìš©"
                    logger.warning(f"âš ï¸ ë…¸ë“œ {node.get('tempId')}ì˜ memoê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •: {default_memo}")
                    node['memo'] = default_memo

            # Kafka ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            kafka_response = {
                "workspaceId": workspace_id,
                "title": result_data['title'],
                "aiSummary": result_data['aiSummary'],
                "status": "SUCCESS",
                "nodes": result_data['nodes']
            }

            logger.info(f"âœ… INITIAL ë¶„ì„ ì™„ë£Œ ({content_type}): {len(result_data['nodes'])}ê°œ ë…¸ë“œ ìƒì„±")
            return kafka_response

        except Exception as e:
            logger.error(f"âŒ INITIAL ë¶„ì„ ì‹¤íŒ¨ ({content_type}): {e}", exc_info=True)
            return {
                "workspaceId": workspace_id,
                "status": "FAILED",
                "error": str(e)
            }

    def process_contextual(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        CONTEXTUAL ë¶„ì„ ì²˜ë¦¬ - 3ê°œ ìì‹ ë…¸ë“œ ìƒì„±
        contentTypeì— ë”°ë¼ VIDEO/IMAGE/TEXT ì²˜ë¦¬

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€
                - workspaceId: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ID
                - nodeId: í™•ì¥í•  ë…¸ë“œ ID
                - nodes: ë¶€ëª¨ ë…¸ë“œë“¤ (ë‚´ ë…¸ë“œ í¬í•¨)
                - contentType: 'VIDEO', 'IMAGE', 'TEXT'

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # workspaceIdë¥¼ intë¡œ ë³€í™˜ (Java longê³¼ í˜¸í™˜)
        workspace_id = int(request['workspaceId'])
        node_id = request['nodeId']
        parent_nodes = request.get('nodes', [])
        content_type = request.get('contentType', 'TEXT')

        logger.info(f"CONTEXTUAL ë¶„ì„ ì‹œì‘: workspaceId={workspace_id}, nodeId={node_id}, "
                   f"contentType={content_type}, ë…¸ë“œ ì²´ì¸ ìˆ˜={len(parent_nodes)}")

        try:
            # 1. nodeIdë¡œ í™•ì¥í•  ë…¸ë“œ ì°¾ê¸°
            target_node = None
            for node in parent_nodes:
                if node.get('nodeId') == node_id or node.get('id') == node_id:
                    target_node = node
                    break

            if not target_node:
                raise ValueError(f"nodeId={node_id}ì¸ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            logger.info(f"í™•ì¥í•  ë…¸ë“œ ì°¾ìŒ: keyword={target_node.get('keyword')}")

            # 2. ContentTypeì— ë”°ë¼ ì»¨í…ì¸  ì¶”ì¶œ
            content_text = ""

            if content_type == 'VIDEO':
                # VIDEO: keywordì—ì„œ YouTube URL ì¶”ì¶œ
                content_url = target_node.get('keyword', '')
                if not content_url:
                    raise ValueError("VIDEO íƒ€ì…ì¸ë° keywordì— URLì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ“¹ VIDEO ë¶„ì„: {content_url}")
                transcript_result = TranscriptExtractor.get_transcript(content_url)
                if transcript_result['success']:
                    content_text = transcript_result['full_text']
                    logger.info(f"âœ… ìë§‰ ì¶”ì¶œ ì„±ê³µ: {len(content_text)}ì")
                else:
                    error_msg = transcript_result.get('error', 'Unknown error')
                    logger.warning(f"âš ï¸  ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")
                    raise ValueError(f"ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {error_msg}")

            elif content_type == 'IMAGE':
                # IMAGE: keywordì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
                content_url = target_node.get('keyword', '')
                if not content_url:
                    raise ValueError("IMAGE íƒ€ì…ì¸ë° keywordì— URLì´ ì—†ìŠµë‹ˆë‹¤")

                logger.info(f"ğŸ–¼ï¸  IMAGE ë¶„ì„: {content_url}")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                image_path = self.image_analyzer.download_image(content_url)
                if not image_path:
                    raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {content_url}")

                logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {image_path}")

                # Vision ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
                vision_prompt = f"""Analyze this image concisely in 3-5 sentences. Focus on unique information only.

Describe:
1. Main visible text and content
2. Key visual elements (charts, diagrams, logos)
3. Core concepts and subtopics

Be concise. Never repeat yourself. Stop after describing main points."""

                content_text = self.vision_analyzer.analyze_image(
                    image=image_path,
                    prompt=vision_prompt,
                    max_tokens=512,
                    temperature=0.3
                )
                logger.info(f"âœ… Vision ë¶„ì„ ì™„ë£Œ: {len(content_text)}ì")
                logger.info(f"ğŸ“„ Vision ë¶„ì„ ë‚´ìš©:\n{content_text}")

                # ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
                self.image_analyzer.cleanup(image_path)

            elif content_type == 'TEXT':
                # TEXT: ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ë§Œ ì‚¬ìš©
                logger.info(f"ğŸ“ TEXT ë¶„ì„: ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ ì‚¬ìš©")
                content_text = ""

            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” contentType: {content_type}")

            # 3. ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ êµ¬ì„±
            context_text = self._build_context_from_parents(parent_nodes)

            # 4. ë§ˆì¸ë“œë§µ ìƒì„± í”„ë¡¬í”„íŠ¸
            system_prompt = """You are an expert at generating exactly 3 child nodes based on parent context.

CRITICAL: You MUST output ONLY valid JSON. No explanations, no markdown, no code blocks.

Output this EXACT JSON structure:
{{
  "nodes": [
    {{"keyword": "Subtopic 1", "memo": "Specific description"}},
    {{"keyword": "Subtopic 2", "memo": "Specific description"}},
    {{"keyword": "Subtopic 3", "memo": "Specific description"}}
  ]
}}

Rules:
1. Generate EXACTLY 3 nodes
2. keyword: 2-5 words, concise
3. memo: 10-30 characters, specific
4. Follow the flow of parent nodes logically
5. Output ONLY JSON, nothing else
"""

            # contentTypeì— ë”°ë¼ user prompt êµ¬ì„±
            if content_text:
                # VIDEO/IMAGE: ì»¨í…ì¸  ë‚´ìš© + ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥
                user_prompt_text = f"""ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥:
{context_text}

ì»¨í…ì¸  ë¶„ì„ ê²°ê³¼:
{content_text[:3000]}

ìœ„ ë¬¸ë§¥ê³¼ ì»¨í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œì˜ 3ê°œ í•˜ìœ„ ë…¸ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."""
            else:
                # TEXT: ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥ë§Œ ì‚¬ìš©
                user_prompt_text = f"""ë¶€ëª¨ ë…¸ë“œ ë¬¸ë§¥:
{context_text}

ìœ„ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œì˜ 3ê°œ í•˜ìœ„ ë…¸ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."""

            # LLM í˜¸ì¶œ (JSON ìƒì„±ì„ ìœ„í•´ ë‚®ì€ temperature)
            logger.info("ğŸ¤– LLMìœ¼ë¡œ 3ê°œ ìì‹ ë…¸ë“œ ìƒì„± ì¤‘...")
            response = self.text_analyzer.generate(
                prompt=user_prompt_text,
                system_prompt=system_prompt,
                max_tokens=1024,
                temperature=0.2,
                top_p=0.85,
                top_k=40,
                repetition_penalty=1.15
            )

            # ì‘ë‹µ ê²€ì¦ ë° ë¡œê¹…
            logger.info(f"ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
            logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:\n{response[:500]}")

            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not response or not response.strip():
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

            # JSON íŒŒì‹± ì „ì²˜ë¦¬
            response = response.strip()

            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```json' in response:
                response = response.split('```json')[1]
            if '```' in response:
                response = response.split('```')[0]

            # JSON ê°ì²´ ì¶”ì¶œ (ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€)
            start_idx = response.find('{')
            end_idx = response.rfind('}')

            if start_idx == -1 or end_idx == -1:
                raise ValueError("ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            response = response[start_idx:end_idx+1].strip()

            # JSON íŒŒì‹± ì‹œë„
            import json
            import re
            try:
                result_data = json.loads(response)
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì˜¤ë¥˜ ë³µêµ¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤: {e}")

                # ì¤‘ë³µ í•„ë“œ ì œê±°: ê°™ì€ ê°ì²´ ë‚´ì—ì„œ ë™ì¼ í•„ë“œê°€ 2ë²ˆ ë‚˜ì˜¤ë©´ ì²« ë²ˆì§¸ë§Œ ìœ ì§€
                # ê° ë…¸ë“œ ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° ì¤‘ë³µ ì œê±°
                try:
                    # ìˆ˜ë™ìœ¼ë¡œ ë…¸ë“œë³„ë¡œ íŒŒì‹±
                    fixed_response = response
                    # ì¤‘ë³µ parentId íŒ¨í„´ ì°¾ê¸°
                    fixed_response = re.sub(
                        r'("parentId"\s*:\s*"[^"]*"),\s*("keyword"[^}]*)"parentId"\s*:\s*"[^"]*"',
                        r'\1, \2',
                        fixed_response
                    )
                    result_data = json.loads(fixed_response)
                    logger.info("âœ… JSON ì˜¤ë¥˜ ë³µêµ¬ ì„±ê³µ")
                except Exception as e2:
                    logger.error(f"JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨!")
                    logger.error(f"ì›ë³¸:\n{response[:500]}")
                    raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e2}")

            # ê²°ê³¼ ê²€ì¦
            if 'nodes' not in result_data:
                raise ValueError("ì‘ë‹µì— nodesê°€ ì—†ìŠµë‹ˆë‹¤")

            if len(result_data['nodes']) != 3:
                logger.warning(f"ë…¸ë“œ ê°œìˆ˜ê°€ 3ê°œê°€ ì•„ë‹™ë‹ˆë‹¤: {len(result_data['nodes'])}ê°œ")
                # 3ê°œë¡œ ìë¥´ê¸° ë˜ëŠ” ì±„ìš°ê¸°
                if len(result_data['nodes']) > 3:
                    result_data['nodes'] = result_data['nodes'][:3]
                elif len(result_data['nodes']) < 3:
                    # ë¶€ì¡±í•œ ë§Œí¼ ê¸°ë³¸ ë…¸ë“œ ì¶”ê°€
                    for i in range(3 - len(result_data['nodes'])):
                        result_data['nodes'].append({
                            "keyword": f"ì¶”ê°€ ì£¼ì œ {i+1}",
                            "memo": "ìƒì„¸ ë‚´ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤"
                        })

            # Kafka ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            kafka_response = {
                "workspaceId": workspace_id,
                "nodeId": node_id,
                "status": "SUCCESS",
                "nodes": result_data['nodes']
            }

            logger.info(f"âœ… CONTEXTUAL ë¶„ì„ ì™„ë£Œ: {len(result_data['nodes'])}ê°œ ë…¸ë“œ ìƒì„±")
            return kafka_response

        except Exception as e:
            logger.error(f"âŒ CONTEXTUAL ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                "workspaceId": workspace_id,
                "status": "FAILED",
                "error": str(e)
            }

    def _build_context_from_parents(self, parent_nodes: List[Dict]) -> str:
        """
        ë¶€ëª¨ ë…¸ë“œë“¤ë¡œë¶€í„° ë¬¸ë§¥ í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            parent_nodes: ë¶€ëª¨ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¬¸ë§¥ í…ìŠ¤íŠ¸
        """
        if not parent_nodes:
            return "(ë¶€ëª¨ ë…¸ë“œ ì—†ìŒ)"

        context_lines = []
        for i, node in enumerate(parent_nodes, 1):
            keyword = node.get('keyword', '')
            memo = node.get('memo', '')
            context_lines.append(f"{i}. {keyword}: {memo}")

        return "\n".join(context_lines)

    def process_organize(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """
        ORGANIZE ì²˜ë¦¬ - text ë…¸ë“œë§Œ ì •ë¦¬, image/videoëŠ” ìœ ì§€

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€
                - workspaceId: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ID
                - nodes: ì „ì²´ ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ (text, image, video)

        Returns:
            ì •ë¦¬ëœ ë§ˆì¸ë“œë§µ ê²°ê³¼
        """
        # workspaceIdë¥¼ intë¡œ ë³€í™˜
        workspace_id = int(request['workspaceId'])
        all_nodes = request.get('nodes', [])

        logger.info(f"ORGANIZE ì‹œì‘: workspaceId={workspace_id}, ì „ì²´ ë…¸ë“œ ìˆ˜={len(all_nodes)}")

        try:
            # text, image, video ë…¸ë“œ ë¶„ë¦¬
            text_nodes = [node for node in all_nodes if node.get('type') == 'text']
            non_text_nodes = [node for node in all_nodes if node.get('type') in ['image', 'video']]

            logger.info(f"ğŸ“ text ë…¸ë“œ: {len(text_nodes)}ê°œ, ğŸ–¼ï¸ image/video ë…¸ë“œ: {len(non_text_nodes)}ê°œ")

            if not text_nodes:
                logger.warning("âš ï¸ text ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
                # analyzedAt ì¶”ê°€
                from datetime import datetime, timezone
                analyzed_at = datetime.now(timezone.utc).astimezone().isoformat()

                return {
                    "workspaceId": workspace_id,
                    "status": "COMPLETED",
                    "nodes": all_nodes,
                    "analyzedAt": analyzed_at
                }

            # LLMìœ¼ë¡œ text ë…¸ë“œ ì •ë¦¬
            logger.info("ğŸ¤– LLMìœ¼ë¡œ text ë…¸ë“œ ì •ë¦¬ ì¤‘...")
            organized_json = self.text_analyzer.organize_mindmap(
                nodes=all_nodes,  # ì „ì²´ ë…¸ë“œ ì „ë‹¬ (ë‚´ë¶€ì—ì„œ textë§Œ í•„í„°ë§)
                max_tokens=4096,
                temperature=0.2
            )

            # ì‘ë‹µ ê²€ì¦ ë° ë¡œê¹…
            logger.info(f"ğŸ“„ LLM ì›ë³¸ ì‘ë‹µ ê¸¸ì´: {len(organized_json)}ì")
            logger.info(f"ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°:\n{organized_json[:500]}")

            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not organized_json or not organized_json.strip():
                raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

            # JSON íŒŒì‹± ì „ì²˜ë¦¬
            organized_json = organized_json.strip()

            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```json' in organized_json:
                organized_json = organized_json.split('```json')[1]
            if '```' in organized_json:
                organized_json = organized_json.split('```')[0]

            # JSON ë°°ì—´ ì¶”ì¶œ (ì²« ë²ˆì§¸ [ ë¶€í„° ë§ˆì§€ë§‰ ] ê¹Œì§€)
            start_idx = organized_json.find('[')
            end_idx = organized_json.rfind(']')

            if start_idx == -1 or end_idx == -1:
                raise ValueError("ì‘ë‹µì—ì„œ JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            organized_json = organized_json[start_idx:end_idx+1].strip()

            # JSON íŒŒì‹±
            import json
            try:
                organized_text_nodes = json.loads(organized_json)
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨!")
                logger.error(f"íŒŒì‹±í•˜ë ¤ë˜ ë¬¸ìì—´:\n{organized_json}")
                raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

            if not isinstance(organized_text_nodes, list):
                raise ValueError(f"LLM ì‘ë‹µì´ ë°°ì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(organized_text_nodes)}")

            logger.info(f"âœ… ì •ë¦¬ëœ text ë…¸ë“œ: {len(organized_text_nodes)}ê°œ (ì›ë³¸: {len(text_nodes)}ê°œ)")

            # ì •ë¦¬ëœ text ë…¸ë“œì— ì›ë³¸ì˜ x, y, color ì •ë³´ ë³µì›
            original_node_map = {node['nodeId']: node for node in text_nodes}

            # ë£¨íŠ¸ ë…¸ë“œ ì¶”ì¶œ (parentId=null)
            root_nodes_map = {node['nodeId']: node for node in text_nodes if node.get('parentId') is None}

            for organized_node in organized_text_nodes:
                node_id = organized_node.get('nodeId')
                if node_id in original_node_map:
                    original = original_node_map[node_id]
                    # x, y, color, type ë³µì›
                    organized_node['x'] = original.get('x', 0.0)
                    organized_node['y'] = original.get('y', 0.0)
                    organized_node['color'] = original.get('color', '#3b82f6')
                    organized_node['type'] = 'text'

                    # ë£¨íŠ¸ ë…¸ë“œ ë³´í˜¸: keywordì™€ memoë¥¼ ì›ë³¸ìœ¼ë¡œ ê°•ì œ ë³µì›
                    if node_id in root_nodes_map:
                        original_keyword = original.get('keyword')
                        original_memo = original.get('memo')
                        current_keyword = organized_node.get('keyword')
                        current_memo = organized_node.get('memo')

                        # keyword ë³µì›
                        if current_keyword != original_keyword:
                            logger.warning(f"ğŸ”’ ë£¨íŠ¸ ë…¸ë“œ {node_id}ì˜ keyword ë³€ê²½ ê°ì§€ - ì›ë³¸ìœ¼ë¡œ ë³µì›: '{current_keyword}' â†’ '{original_keyword}'")
                            organized_node['keyword'] = original_keyword

                        # memo ë³µì›
                        if current_memo != original_memo:
                            logger.warning(f"ğŸ”’ ë£¨íŠ¸ ë…¸ë“œ {node_id}ì˜ memo ë³€ê²½ ê°ì§€ - ì›ë³¸ìœ¼ë¡œ ë³µì›")
                            organized_node['memo'] = original_memo

                        # parentIdë„ null ìœ ì§€
                        if organized_node.get('parentId') is not None:
                            logger.warning(f"ğŸ”’ ë£¨íŠ¸ ë…¸ë“œ {node_id}ì˜ parentId ë³€ê²½ ê°ì§€ - nullë¡œ ë³µì›")
                            organized_node['parentId'] = None

                else:
                    logger.warning(f"âš ï¸ nodeId={node_id}ì¸ ì›ë³¸ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # text + non-text ë…¸ë“œ ë³‘í•©
            final_nodes = organized_text_nodes + non_text_nodes

            logger.info(f"ğŸ“Š ìµœì¢… ë…¸ë“œ ìˆ˜: {len(final_nodes)}ê°œ")

            # analyzedAt ì¶”ê°€
            from datetime import datetime, timezone
            analyzed_at = datetime.now(timezone.utc).astimezone().isoformat()

            # Kafka ì‘ë‹µ í˜•ì‹
            kafka_response = {
                "workspaceId": workspace_id,
                "status": "COMPLETED",
                "nodes": final_nodes,
                "analyzedAt": analyzed_at
            }

            logger.info(f"âœ… ORGANIZE ì™„ë£Œ: {len(final_nodes)}ê°œ ë…¸ë“œ ë°˜í™˜")
            return kafka_response

        except Exception as e:
            logger.error(f"âŒ ORGANIZE ì‹¤íŒ¨: {e}", exc_info=True)
            from datetime import datetime, timezone
            analyzed_at = datetime.now(timezone.utc).astimezone().isoformat()

            return {
                "workspaceId": workspace_id,
                "status": "FAILED",
                "nodes": all_nodes,  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
                "analyzedAt": analyzed_at,
                "error": str(e)
            }

    def process_request(self, request: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Kafka ìš”ì²­ ì²˜ë¦¬ (ë¶„ì„ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°)

        Args:
            request: Kafka ìš”ì²­ ë©”ì‹œì§€

        Returns:
            ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None
        """
        analysis_type = request.get('analysisType')

        if analysis_type == 'INITIAL':
            return self.process_initial(request)
        elif analysis_type == 'CONTEXTUAL':
            return self.process_contextual(request)
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¶„ì„ íƒ€ì…: {analysis_type}")
            return {
                "workspaceId": request.get('workspaceId'),
                "status": "FAILED",
                "error": f"Unknown analysis type: {analysis_type}"
            }
