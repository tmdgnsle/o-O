#!/usr/bin/env python3
"""
Llama 3.1 ì˜êµ¬ ì±„íŒ… (ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ)
ì²« ë¡œë”©ë§Œ ëŠë¦¬ê³ , ì´í›„ì—” ë¹ ë¥´ê²Œ ëŒ€í™” ê°€ëŠ¥
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer
from threading import Thread
import os
from dotenv import load_dotenv
import sys

load_dotenv()

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥ (í•œ ë²ˆë§Œ ë¡œë“œ)
_model = None
_tokenizer = None


def load_model_once(model_name="meta-llama/Llama-3.1-8B-Instruct", quantization="int4"):
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ì— ì €ì¥)"""
    global _model, _tokenizer

    if _model is not None and _tokenizer is not None:
        print("âœ… ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš© (ë¹ ë¦„!)")
        return _model, _tokenizer

    print("="*60)
    print("ğŸ”„ Llama 3.1 ëª¨ë¸ ë¡œë”© ì¤‘... (ì²« ë¡œë”©ë§Œ ëŠë¦¼)")
    print("="*60)
    print(f"ğŸ“¦ ëª¨ë¸: {model_name}")
    print(f"âš™ï¸  ì–‘ìí™”: {quantization}")
    print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")
    print()

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("1/2 í† í¬ë‚˜ì´ì € ë¡œë”©...")
    _tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        token=os.getenv("HUGGINGFACE_TOKEN")
    )

    # ëª¨ë¸ ë¡œë“œ
    print("2/2 ëª¨ë¸ ë¡œë”©...")
    if quantization == "int4" and torch.cuda.is_available():
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        _model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            token=os.getenv("HUGGINGFACE_TOKEN"),
            low_cpu_mem_usage=True,
            torch_dtype=torch.bfloat16,
        )
    elif quantization == "int8" and torch.cuda.is_available():
        quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        _model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            token=os.getenv("HUGGINGFACE_TOKEN"),
            low_cpu_mem_usage=True,
        )
    else:
        _model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            token=os.getenv("HUGGINGFACE_TOKEN"),
            low_cpu_mem_usage=True,
        )

    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    print("="*60)
    print()

    return _model, _tokenizer


def generate_stream(prompt, conversation_history=None, max_new_tokens=512, temperature=0.7):
    """ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (ì „ì—­ ëª¨ë¸ ì‚¬ìš©)"""
    model, tokenizer = load_model_once()

    # ë©”ì‹œì§€ í¬ë§·
    if conversation_history is None:
        messages = [
            {"role": "system", "content": "You are a helpful AI assistant. You can speak Korean fluently."},
            {"role": "user", "content": prompt}
        ]
    else:
        messages = conversation_history + [{"role": "user", "content": prompt}]

    # í† í¬ë‚˜ì´ì € í…œí”Œë¦¿ ì ìš©
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # í† í°í™”
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    # ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„±
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True
    )

    # ìƒì„± íŒŒë¼ë¯¸í„°
    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        do_sample=True,
        top_p=0.9,
        repetition_penalty=1.1,
    )

    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ì‹œì‘
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
    print("ğŸ’¬ ì‘ë‹µ:\n")
    response_text = ""
    for text in streamer:
        print(text, end="", flush=True)
        response_text += text

    print("\n")
    thread.join()

    return response_text


def chat_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    print("\n" + "="*60)
    print("ğŸ¯ Llama 3.1 ê³ ì† ì±„íŒ… ëª¨ë“œ")
    print("="*60)
    print("ğŸ’¡ ì²« ì‹¤í–‰ë§Œ ëŠë¦¬ê³ , ì´í›„ì—” ë¹ ë¦…ë‹ˆë‹¤!")
    print("ğŸ’¡ 'quit' ë˜ëŠ” 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ")
    print("ğŸ’¡ 'clear' ì…ë ¥ ì‹œ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")
    print("="*60)
    print()

    # ì²« ë¡œë”© (ì—¬ê¸°ì„œë§Œ 1ë¶„ ê±¸ë¦¼)
    load_model_once()

    # ëŒ€í™” íˆìŠ¤í† ë¦¬
    conversation_history = [
        {"role": "system", "content": "You are a helpful AI assistant. You can speak Korean fluently."}
    ]
    turn = 0

    print("âœ… ì¤€ë¹„ ì™„ë£Œ! ì´ì œ ë¹ ë¥´ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    while True:
        try:
            user_input = input("ğŸ“ ì§ˆë¬¸: ").strip()

            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print(f"\nâœ… ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì´ {turn}í„´)")
                break

            if user_input.lower() in ['clear', 'ì´ˆê¸°í™”']:
                conversation_history = [
                    {"role": "system", "content": "You are a helpful AI assistant. You can speak Korean fluently."}
                ]
                turn = 0
                print("ğŸ”„ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                continue

            if not user_input:
                continue

            turn += 1
            print("-" * 60)

            # ì‘ë‹µ ìƒì„± (ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš© - ë¹ ë¦„!)
            response = generate_stream(
                user_input,
                conversation_history=conversation_history,
                max_new_tokens=512,
                temperature=0.7
            )

            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            conversation_history.append({"role": "user", "content": user_input})
            conversation_history.append({"role": "assistant", "content": response})

            print(f"ğŸ’¬ (í„´ {turn}, ë©”ì‹œì§€: {len(conversation_history)}ê°œ)")

        except KeyboardInterrupt:
            print(f"\n\nâš ï¸  ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì´ {turn}í„´)")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    try:
        chat_mode()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
