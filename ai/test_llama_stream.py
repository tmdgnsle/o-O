#!/usr/bin/env python3
"""
Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
í…ìŠ¤íŠ¸ ìƒì„±ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥
"""
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer
from threading import Thread
import os
from dotenv import load_dotenv
import sys

load_dotenv()


class LlamaStreamer:
    """Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° ìƒì„±ê¸°"""

    def __init__(self, model_name="meta-llama/Llama-3.1-8B-Instruct", quantization="int4"):
        """
        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„
            quantization: "int4", "int8", "fp16", None
        """
        print(f"ğŸ”„ Llama 3.1 ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“¦ ëª¨ë¸: {model_name}")
        print(f"âš™ï¸  ì–‘ìí™”: {quantization}")
        print(f"ğŸ’¾ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            token=os.getenv("HUGGINGFACE_TOKEN")
        )

        # ì–‘ìí™” ì„¤ì •
        if quantization == "int4" and torch.cuda.is_available():
            print("âš™ï¸  INT4 ì–‘ìí™” (VRAM ~4GB)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        elif quantization == "int8" and torch.cuda.is_available():
            print("âš™ï¸  INT8 ì–‘ìí™” (VRAM ~8GB)")
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )
        else:
            print("âš™ï¸  FP16 (VRAM ~16GB)")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                token=os.getenv("HUGGINGFACE_TOKEN")
            )

        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")

    def generate_stream(self, prompt, max_new_tokens=512, temperature=0.7, conversation_history=None):
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±

        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„ (0.1~1.0)
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ë¦¬ìŠ¤íŠ¸)
        """
        # ë©”ì‹œì§€ í¬ë§· (Llama 3.1 Chat Template)
        if conversation_history is None:
            messages = [
                {"role": "system", "content": "You are a helpful AI assistant. You can speak Korean fluently."},
                {"role": "user", "content": prompt}
            ]
        else:
            messages = conversation_history + [{"role": "user", "content": prompt}]

        # í† í¬ë‚˜ì´ì € í…œí”Œë¦¿ ì ìš©
        input_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # í† í°í™”
        inputs = self.tokenizer(input_text, return_tensors="pt").to(self.model.device)

        # ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„±
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )

        # ìƒì„± íŒŒë¼ë¯¸í„°
        generation_kwargs = dict(
            inputs,
            streamer=streamer,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
        )

        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒì„± ì‹œì‘
        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ ë° ì‘ë‹µ ìˆ˜ì§‘
        print("ğŸ’¬ ì‘ë‹µ:\n")
        response_text = ""
        for text in streamer:
            print(text, end="", flush=True)
            response_text += text

        print("\n")
        thread.join()

        return response_text


def test_simple_prompt():
    """ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¯ Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ - ê°„ë‹¨í•œ ì§ˆë¬¸")
    print("="*60)

    llama = LlamaStreamer(quantization="int4")

    prompt = "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”? 3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    print(f"ğŸ“ ì§ˆë¬¸: {prompt}\n")
    print("-" * 60)

    llama.generate_stream(prompt, max_new_tokens=200, temperature=0.7)


def test_creative_writing():
    """ì°½ì‘ ê¸€ì“°ê¸° í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¯ Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ - ì°½ì‘ ê¸€ì“°ê¸°")
    print("="*60)

    llama = LlamaStreamer(quantization="int4")

    prompt = "ìš°ì£¼ íƒí—˜ì„ ì£¼ì œë¡œ ì§§ì€ ì´ì•¼ê¸°ë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. (200ì ì´ë‚´)"
    print(f"ğŸ“ ì§ˆë¬¸: {prompt}\n")
    print("-" * 60)

    llama.generate_stream(prompt, max_new_tokens=300, temperature=0.9)


def test_code_generation():
    """ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¯ Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ - ì½”ë“œ ìƒì„±")
    print("="*60)

    llama = LlamaStreamer(quantization="int4")

    prompt = "Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
    print(f"ğŸ“ ì§ˆë¬¸: {prompt}\n")
    print("-" * 60)

    llama.generate_stream(prompt, max_new_tokens=300, temperature=0.3)


def test_summarization():
    """ìš”ì•½ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("ğŸ¯ Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ - í…ìŠ¤íŠ¸ ìš”ì•½")
    print("="*60)

    llama = LlamaStreamer(quantization="int4")

    long_text = """
    ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥, ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì»´í“¨í„° ì‹œìŠ¤í…œì´ë‹¤.
    AIëŠ” í¬ê²Œ ì•½í•œ AIì™€ ê°•í•œ AIë¡œ ë‚˜ë‰œë‹¤. ì•½í•œ AIëŠ” íŠ¹ì • ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆìœ¼ë©°,
    í˜„ì¬ ëŒ€ë¶€ë¶„ì˜ AI ì‹œìŠ¤í…œì´ ì´ì— í•´ë‹¹í•œë‹¤. ë°˜ë©´ ê°•í•œ AIëŠ” ì¸ê°„ ìˆ˜ì¤€ì˜ ì§€ëŠ¥ì„ ê°€ì§„ ì‹œìŠ¤í…œìœ¼ë¡œ,
    ì•„ì§ ì‹¤í˜„ë˜ì§€ ì•Šì•˜ë‹¤. ìµœê·¼ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ AIëŠ” ì´ë¯¸ì§€ ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬,
    ìŒì„± ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤.
    """

    prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{long_text}"
    print(f"ğŸ“ ì§ˆë¬¸: (ìš”ì•½ ìš”ì²­)\n")
    print("-" * 60)

    llama.generate_stream(prompt, max_new_tokens=100, temperature=0.3)


def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ (ëŒ€í™” ê¸°ë¡ í¬í•¨)"""
    print("\n" + "="*60)
    print("ğŸ¯ Llama 3.1 ëŒ€í™”í˜• ëª¨ë“œ (ì´ì „ ëŒ€í™” ê¸°ì–µ)")
    print("="*60)
    print("ğŸ’¡ 'quit' ë˜ëŠ” 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œ")
    print("ğŸ’¡ 'clear' ì…ë ¥ ì‹œ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”\n")

    llama = LlamaStreamer(quantization="int4")

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    conversation_history = [
        {"role": "system", "content": "You are a helpful AI assistant. You can speak Korean fluently."}
    ]

    turn = 0

    while True:
        try:
            user_input = input("\nğŸ“ ì§ˆë¬¸: ").strip()

            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print(f"\nâœ… ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì´ {turn}í„´)")
                break

            if user_input.lower() in ['clear', 'ì´ˆê¸°í™”']:
                conversation_history = [
                    {"role": "system", "content": "You are a helpful AI assistant. You can speak Korean fluently."}
                ]
                turn = 0
                print("ğŸ”„ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                continue

            if not user_input:
                continue

            turn += 1
            print("-" * 60)

            # ì‘ë‹µ ìƒì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨)
            response = llama.generate_stream(
                user_input,
                max_new_tokens=512,
                temperature=0.7,
                conversation_history=conversation_history
            )

            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            conversation_history.append({"role": "user", "content": user_input})
            conversation_history.append({"role": "assistant", "content": response})

            print(f"ğŸ’¬ (í„´ {turn}, ëŒ€í™” ê¸°ë¡: {len(conversation_history)} ë©”ì‹œì§€)")

        except KeyboardInterrupt:
            print(f"\n\nâš ï¸  ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. (ì´ {turn}í„´)")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Llama 3.1 ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸")
    parser.add_argument(
        "--mode",
        choices=["simple", "creative", "code", "summary", "chat"],
        default="simple",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ"
    )

    args = parser.parse_args()

    try:
        if args.mode == "simple":
            test_simple_prompt()
        elif args.mode == "creative":
            test_creative_writing()
        elif args.mode == "code":
            test_code_generation()
        elif args.mode == "summary":
            test_summarization()
        elif args.mode == "chat":
            interactive_mode()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
